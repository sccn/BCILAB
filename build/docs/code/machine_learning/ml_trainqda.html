<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of ml_trainqda</title>
  <meta name="keywords" content="ml_trainqda">
  <meta name="description" content="Learn a non-linear predictive model by (regularized) Quadratic Discriminant Analysis.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../index.html">Home</a> &gt;  <a href="#">code</a> &gt; <a href="index.html">machine_learning</a> &gt; ml_trainqda.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../index.html"><img alt="<" border="0" src="../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for code/machine_learning&nbsp;<img alt=">" border="0" src="../../right.png"></a></td></tr></table>-->

<h1>ml_trainqda

</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>Learn a non-linear predictive model by (regularized) Quadratic Discriminant Analysis.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>function model = ml_trainqda(varargin) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Learn a non-linear predictive model by (regularized) Quadratic Discriminant Analysis.
 Model = ml_trainqda(Trials, Targets, Lambda, Kappa Options...)

 Quadratic discriminant analysis [1] is a (quadratic) generalization of linear discriminant
 analysis, and practically all remarks from ml_trainlda apply to this classifier, as well. However,
 QDA is siginificantly more prone to overfitting than is LDA, since it relaxes the assumption that
 the covariance structure of each class is identical. For this reason, it has twice as many
 parameters, and is also approx. twice as slow. Aside from that, QDA is the simplest non-linear
 classifier and is interesting especially when it is known that one distribution is shaped
 significantly different from the other (e.g. in one-vs-rest votings, or in baseline-versus-signal
 classification).

 In:
   Trials       : training data, as in ml_train

   Targets      : target variable, as in ml_train

   Lambda       : optional per-class covariance regularization parameter, reasonable range: 0:0.1:1, greater is stronger
                  requires that the regularization option is set to either 'shrinkage' or 'independence' (default: [])
   Kappa        : between-class covariance regularization parameter, reasonable range: 0:0.1:1, greater is stronger (default: [])

   Options  : optional name-value parameters to control the training details:
              'weight_cov' -&gt; 0/1, take unequal class priors into account for covariance calculation
                              default: 0
              'regularization' -&gt; 'shrinkage': covariance shrinkage, depends on lambda 
                                  'independence': feature independence, depends on lambda
                                  'auto': analytical covariance shrinkage, lambda is ignored (default)
 Out:
   Model   : a quadratic model; q is the quadratic weights, l is the linear weights, b is the bias; 
             classes indicates the class labels which the model predicts; sc_info holds scale parameters

 Examples:
   % learn a shrinkage QDA model
   model = ml_trainqda(trials,targets);

   % take unequal class priors into account for both the bias and the covariance matrix
   model = ml_trainqda(trials,targets,[],[],'weight_bias',1,'weight_cov',1);

   % learn a shrinkage QDA model which blends covariance estimates between classes, using a 
   % regularization parameter
   model = utl_searchmodel({trials,target},'args',{{'qda',[],search(0:0.1:1)}})

   % use a different type of regularization, which controls feature independence
   model = utl_searchmodel({trials,target},'args',{{'qda',search(0:0.1:1),[],'regularization','independence'}})
   
   % as before, but search for the optimal blending parameter at the same time (slow!)
   model = utl_searchmodel({trials,target},'args',{{'qda',search(0:0.1:1),search(0:0.1:1),'regularization','independence'}})

 See also:
   <a href="ml_predictqda.html" class="code" title="function pred = ml_predictqda(trials, model)">ml_predictqda</a>

 References:
   [1] Friedman, J. &quot;Regularized discriminant analysis.&quot;
       Journal of the American Statistical Association 84, 405 (1989), 175, 165.

                           Christian Kothe, Swartz Center for Computational Neuroscience, UCSD
                           2010-04-03</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">

<li><a href="ml_predictqda.html" class="code" title="function pred = ml_predictqda(trials, model)">ml_predictqda</a>	Prediction function for Quadratic Discriminant Analysis.</li>
<li><a href="ml_trainqda.html" class="code" title="function model = ml_trainqda(varargin)">ml_trainqda</a>	Learn a non-linear predictive model by (regularized) Quadratic Discriminant Analysis.</li>
<li><a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>	Internal meta-algorithm for voting. Used by other machine learning functions.</li>
</ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">

<li><a href="ml_trainqda.html" class="code" title="function model = ml_trainqda(varargin)">ml_trainqda</a>	Learn a non-linear predictive model by (regularized) Quadratic Discriminant Analysis.</li>
</ul>
<!-- crossreference -->






<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function model = ml_trainqda(varargin)</a>
0002 <span class="comment">% Learn a non-linear predictive model by (regularized) Quadratic Discriminant Analysis.</span>
0003 <span class="comment">% Model = ml_trainqda(Trials, Targets, Lambda, Kappa Options...)</span>
0004 <span class="comment">%</span>
0005 <span class="comment">% Quadratic discriminant analysis [1] is a (quadratic) generalization of linear discriminant</span>
0006 <span class="comment">% analysis, and practically all remarks from ml_trainlda apply to this classifier, as well. However,</span>
0007 <span class="comment">% QDA is siginificantly more prone to overfitting than is LDA, since it relaxes the assumption that</span>
0008 <span class="comment">% the covariance structure of each class is identical. For this reason, it has twice as many</span>
0009 <span class="comment">% parameters, and is also approx. twice as slow. Aside from that, QDA is the simplest non-linear</span>
0010 <span class="comment">% classifier and is interesting especially when it is known that one distribution is shaped</span>
0011 <span class="comment">% significantly different from the other (e.g. in one-vs-rest votings, or in baseline-versus-signal</span>
0012 <span class="comment">% classification).</span>
0013 <span class="comment">%</span>
0014 <span class="comment">% In:</span>
0015 <span class="comment">%   Trials       : training data, as in ml_train</span>
0016 <span class="comment">%</span>
0017 <span class="comment">%   Targets      : target variable, as in ml_train</span>
0018 <span class="comment">%</span>
0019 <span class="comment">%   Lambda       : optional per-class covariance regularization parameter, reasonable range: 0:0.1:1, greater is stronger</span>
0020 <span class="comment">%                  requires that the regularization option is set to either 'shrinkage' or 'independence' (default: [])</span>
0021 <span class="comment">%   Kappa        : between-class covariance regularization parameter, reasonable range: 0:0.1:1, greater is stronger (default: [])</span>
0022 <span class="comment">%</span>
0023 <span class="comment">%   Options  : optional name-value parameters to control the training details:</span>
0024 <span class="comment">%              'weight_cov' -&gt; 0/1, take unequal class priors into account for covariance calculation</span>
0025 <span class="comment">%                              default: 0</span>
0026 <span class="comment">%              'regularization' -&gt; 'shrinkage': covariance shrinkage, depends on lambda</span>
0027 <span class="comment">%                                  'independence': feature independence, depends on lambda</span>
0028 <span class="comment">%                                  'auto': analytical covariance shrinkage, lambda is ignored (default)</span>
0029 <span class="comment">% Out:</span>
0030 <span class="comment">%   Model   : a quadratic model; q is the quadratic weights, l is the linear weights, b is the bias;</span>
0031 <span class="comment">%             classes indicates the class labels which the model predicts; sc_info holds scale parameters</span>
0032 <span class="comment">%</span>
0033 <span class="comment">% Examples:</span>
0034 <span class="comment">%   % learn a shrinkage QDA model</span>
0035 <span class="comment">%   model = ml_trainqda(trials,targets);</span>
0036 <span class="comment">%</span>
0037 <span class="comment">%   % take unequal class priors into account for both the bias and the covariance matrix</span>
0038 <span class="comment">%   model = ml_trainqda(trials,targets,[],[],'weight_bias',1,'weight_cov',1);</span>
0039 <span class="comment">%</span>
0040 <span class="comment">%   % learn a shrinkage QDA model which blends covariance estimates between classes, using a</span>
0041 <span class="comment">%   % regularization parameter</span>
0042 <span class="comment">%   model = utl_searchmodel({trials,target},'args',{{'qda',[],search(0:0.1:1)}})</span>
0043 <span class="comment">%</span>
0044 <span class="comment">%   % use a different type of regularization, which controls feature independence</span>
0045 <span class="comment">%   model = utl_searchmodel({trials,target},'args',{{'qda',search(0:0.1:1),[],'regularization','independence'}})</span>
0046 <span class="comment">%</span>
0047 <span class="comment">%   % as before, but search for the optimal blending parameter at the same time (slow!)</span>
0048 <span class="comment">%   model = utl_searchmodel({trials,target},'args',{{'qda',search(0:0.1:1),search(0:0.1:1),'regularization','independence'}})</span>
0049 <span class="comment">%</span>
0050 <span class="comment">% See also:</span>
0051 <span class="comment">%   ml_predictqda</span>
0052 <span class="comment">%</span>
0053 <span class="comment">% References:</span>
0054 <span class="comment">%   [1] Friedman, J. &quot;Regularized discriminant analysis.&quot;</span>
0055 <span class="comment">%       Journal of the American Statistical Association 84, 405 (1989), 175, 165.</span>
0056 <span class="comment">%</span>
0057 <span class="comment">%                           Christian Kothe, Swartz Center for Computational Neuroscience, UCSD</span>
0058 <span class="comment">%                           2010-04-03</span>
0059     
0060 arg_define([0 4],varargin, <span class="keyword">...</span>
0061     arg_norep(<span class="string">'trials'</span>), <span class="keyword">...</span>
0062     arg_norep(<span class="string">'targets'</span>), <span class="keyword">...</span>
0063     arg({<span class="string">'lambda'</span>,<span class="string">'Lambda'</span>}, [], [0 1], <span class="string">'Within-class covariance regularization parameter. Reasonable range: 0:0.1:1 - greater is stronger. Requires that the regularization mode is set to either &quot;shrinkage&quot; or &quot;independence&quot;.'</span>), <span class="keyword">...</span>
0064     arg({<span class="string">'kappav'</span>,<span class="string">'Kappa'</span>,<span class="string">'kappa'</span>}, [], [0 1], <span class="string">'Between-class covariance regularization parameter. Reasonable range: 0:0.1:1 - greater is stronger. Requires that the regularization mode is set to either &quot;shrinkage&quot; or &quot;independence&quot;.'</span>), <span class="keyword">...</span>
0065     arg({<span class="string">'regularization'</span>,<span class="string">'Regularizer'</span>}, <span class="string">'auto'</span>, {<span class="string">'auto'</span>,<span class="string">'shrinkage'</span>,<span class="string">'independence'</span>}, <span class="string">'Regularization type. Regularizes the robustness / flexibility of covariance estimates. Auto is analytical covariance shrinkage, shrinkage is shrinkage as selected via lambda, and independence is feature independence, also selected via lambda.'</span>), <span class="keyword">...</span>
0066     arg({<span class="string">'weight_cov'</span>,<span class="string">'WeightedCov'</span>}, false, [], <span class="string">'Account for class priors in covariance. If you do have unequal probabilities for the different classes, it makes sense to enable this.'</span>), <span class="keyword">...</span>
0067     arg({<span class="string">'votingScheme'</span>,<span class="string">'VotingScheme'</span>},<span class="string">'1vR'</span>,{<span class="string">'1v1'</span>,<span class="string">'1vR'</span>},<span class="string">'Voting scheme. If multi-class classification is used, this determine how binary classifiers are arranged to solve the multi-class problem. 1v1 gets slow for large numbers of classes (as all pairs are tested), but can be more accurate than 1vR.'</span>), <span class="keyword">...</span>
0068     arg_deprecated({<span class="string">'weight_bias'</span>,<span class="string">'WeightedBias'</span>},false));
0069 
0070 
0071 <span class="comment">% find the class labels</span>
0072 classes = unique(targets);
0073 <span class="keyword">if</span> length(classes) &gt; 2
0074     <span class="comment">% learn a voting arrangement</span>
0075     model = <a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>(trials,targets,votingScheme,@<a href="ml_trainqda.html" class="code" title="function model = ml_trainqda(varargin)">ml_trainqda</a>,@<a href="ml_predictqda.html" class="code" title="function pred = ml_predictqda(trials, model)">ml_predictqda</a>,varargin{:}); <span class="comment">%#ok&lt;*NODEF&gt;</span>
0076 <span class="keyword">elseif</span> length(classes) == 1
0077     error(<span class="string">'BCILAB:only_one_class'</span>,<span class="string">'Your training data set has no trials for one of your classes; you need at least two classes to train a classifier.\n\nThe most likely reasons are that one of your target markers does not occur in the data, or that all your trials of a particular class are concentrated in a single short segment of your data (10 or 20 percent). The latter would be a problem with the experiment design.'</span>);
0078 <span class="keyword">else</span>
0079     <span class="comment">% pre-prune degenerate features</span>
0080     retain = true(1,size(trials,2));
0081     <span class="keyword">for</span> c = 1:length(classes)
0082         X = trials(targets==classes(c),:);
0083         n{c} = size(X,1);
0084         mu{c} = mean(X);
0085         v{c} = var(X);
0086         retain = retain &amp; isfinite(mu{c}) &amp; isfinite(v{c}) &amp; v{c} &gt; eps;
0087     <span class="keyword">end</span>    
0088     <span class="comment">% apply feature mask...</span>
0089     trials = trials(:,retain);
0090     
0091     <span class="comment">% scale the data (to keep the numerics happy)</span>
0092     sc_info = hlp_findscaling(trials,<span class="string">'std'</span>);
0093     trials = hlp_applyscaling(trials,sc_info);
0094         
0095     <span class="comment">% estimate distribution of each class...</span>
0096     <span class="keyword">for</span> c = 1:length(classes)
0097         <span class="comment">% get mean and covariance</span>
0098         X = trials(targets==classes(c),:);
0099         n{c} = size(X,1);
0100         mu{c} = mean(X);
0101         <span class="keyword">if</span> strcmp(regularization,<span class="string">'auto'</span>)
0102             sig{c} = cov_shrink(X);
0103         <span class="keyword">else</span>
0104             sig{c} = cov(X);
0105             <span class="keyword">if</span> ~isempty(lambda)
0106                 <span class="comment">% lambda-dependent regularization</span>
0107                 <span class="keyword">if</span> strcmp(regularization,<span class="string">'independence'</span>)
0108                     sig{c} = (1-lambda)*sig{c} + lambda * diag(diag(sig{c}));
0109                 <span class="keyword">elseif</span> strcmp(regularization,<span class="string">'shrinkage'</span>)
0110                     sig{c} = (1-lambda)*sig{c} + lambda*eye(length(sig{c}))*abs(mean(diag(sig{c})));
0111                 <span class="keyword">else</span>
0112                     error(<span class="string">'unknown regularization mode'</span>);
0113                 <span class="keyword">end</span>
0114             <span class="keyword">end</span>
0115         <span class="keyword">end</span>
0116     <span class="keyword">end</span>
0117     
0118     <span class="keyword">if</span> ~isempty(kappav)
0119         <span class="comment">% regularize the covariance matrices w.r.t. each other</span>
0120         ns = quickif(weight_cov,n,{1 1});
0121         sigma = (sig{1}*ns{1} + sig{2}*ns{2}) / (ns{1}+ns{2});
0122         sig{1} = sig{1} * (1-kappav) + sigma*kappav;
0123         sig{2} = sig{2} * (1-kappav) + sigma*kappav;
0124     <span class="keyword">end</span>
0125     
0126     <span class="comment">% compute the model</span>
0127     model = struct(<span class="string">'c'</span>,{1/2*(logdet(sig{1})-logdet(sig{2})) + 1/2*((mu{1}/sig{1})*mu{1}' - (mu{2}/sig{2})*mu{2}')}, <span class="keyword">...</span>
0128         <span class="string">'l'</span>,{mu{1}/sig{1} - mu{2}/sig{2}}, <span class="string">'q'</span>,{-1/2*(inv(sig{1}) - inv(sig{2}))}, <span class="string">'sc_info'</span>,{sc_info}, <span class="string">'classes'</span>,{classes}, <span class="string">'featuremask'</span>,{retain});
0129 <span class="keyword">end</span></pre></div>

<hr><address>Generated on Wed 19-Aug-2015 18:06:23 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>