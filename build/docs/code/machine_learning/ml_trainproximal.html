<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of ml_trainproximal</title>
  <meta name="keywords" content="ml_trainproximal">
  <meta name="description" content="Learn a linear probabilistic model proximal splitting methods.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../index.html">Home</a> &gt;  <a href="#">code</a> &gt; <a href="index.html">machine_learning</a> &gt; ml_trainproximal.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../index.html"><img alt="<" border="0" src="../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for code/machine_learning&nbsp;<img alt=">" border="0" src="../../right.png"></a></td></tr></table>-->

<h1>ml_trainproximal

</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>Learn a linear probabilistic model proximal splitting methods.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>function model = ml_trainproximal(varargin) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Learn a linear probabilistic model proximal splitting methods.
 Model = ml_trainproximal(Trials, Targets, Lambdas, Options...)

 This function allows to implement linear or logistic regression using a variety of regularization
 terms and combinations thereof using proximal splitting [1].

 In:
   Trials       : training data matrix, as in ml_train

   Targets      : 1d target variable vector, as in ml_train

   LossType     : loss function to be used,
                  'logistic' for classification (default)
                  'squared' for regression
                  'hyperbolic-secant' special-purpose for super-Gaussian estimation

   Regularizers : Definition of the regularization terms. Any combination of terms is permitted.


   Options  : optional name-value parameters to control the training details:

               'regweights' : Weights of the regularizers. This is a vector of (relative) regularization
                              parameters. If [] set to 1/N for N regularizers. (default: [])
                              Can also be a cell array of (normalized) weight vectors; in this case 
                              it it simultaneously optimized together with the lambdas.

               'solverOptions' : cell array of name-value pairs to control how the outer ADMM solver
                                 behaves

                    'abs_tol' : Absolute tolerance criterion. (default: 10e-4)

                    'rel_tol' : Relative tolerance criterion. (default: 10e-3)

                    'maxit' : Maximum number of iterations. (default: 1000)

                    'rho' : Initial coupling parameter. For proximal algorithms this is the coupling strength
                            between the terms between updates. Increasing this can improve the convergence
                            speed but too strong values can prevent any convergence. (default: 1)

                    'rho_update' : Update Rho. Whether to update rho dynamically according to 3.4.1 in [2].
                                   Note, this can sometimes cause r_norm, s_norm to &quot;blow up&quot;. (default: true)

                    'rho_cutoff' : Rho update threshold. (default: 10)

                    'rho_incr' : Rho update increment factor. (default: 2)

                    'rho_decr' : Rho update decrement factor. (default: 2)

               'lbfgsOptions' : cell array of name-value pairs to control how the inner LFBGS solver
                                behaves

                    'm' : LBFGS history length. The number of corrections to approximate the inverse
                          hessian matrix. (default: 6)

                    'epsilon' : Tolerance criterion.  A minimization terminates when ||g|| &lt; epsilon*max(1,||x||).
                                (default: 1e-3)

                    'past' : Distance for delta-based convergence test. (default: 0)

                    'delta' : Delta for convergence test. (default: 1e-5)

                    'MaxIter' : Maximum number of iterations. (default: 10)

                    'linesearch' : The line search algorithm. Can be any of the following:
                                   {'more_thuente','backtracking_armijo','backtracking_wolfe','backtracking_strong_wolfe'}
                                   (default: more_thuente)

                    'max_linesearch' : Maximum number of trials for the line search. (default: 40)

                    'min_step' : Minimum step of the line search. (default: 1e-20)

                    'max_step': Maximum step of the line search routine. (default: 1e20)

                    'ftol' : Line search tolerance F. A parameter to control the accuracy of the
                             line search routine. (default: 1e-4)

                    'wolfe' : Coefficient for the Wolfe condition. (default: 0.9)

                    'gtol' : Line search tolerance G. A parameter to control the accuracy of the
                             line search routine. (default: 0.9)

                    'xtol' : Machine precision for floating-point values. (default: 1e-16)

                    'DerivativeCheck' : Derivative check using finite differences. (default: 'off')

                    'Display' : Options for displaying progress. (default: 'none')

               'lambdaSearch' : cell array of name-value pairs governing the regularization path search

                   'lambdas' : Regulariation parameters. Controls the sparsity/simplicity of the result.
                               Typically, this is an interval to scan. (default: 2.^(3:-0.25:-5))

                   'nfolds' : Cross-validation folds. The cross-validation is used to determine the best
                              regularization parameter value (default: 5)

                   'foldmargin' : Margin (in trials) between folds. This is the number of trials omitted
                                  between training and test sets. (default: 5)

                   'cvmetric' : metric to use for parameter optimization; can be any of those supported by
                                ml_calcloss. In particular, 'auc' is a good idea if the classification
                                task is between highly imbalanced classes. (default: '' = auto-determine)

                   'return_regpath' : Return the entire regularization path. If false, only the best model will
                                      be returned. (default: true)


               'scaling': pre-scaling of the data (see hlp_findscaling for options) (default: 'std')

               'data_weights': dataset weights; optional vector of weights for each task in the
                               training data (one element per task in a multi-task learning setting) (default: [])

               'includebias': whether to include a bias param (default: true)

               'verbosity': verbosity level, 0-3 (0=no output)

 Out:
   Models   : a predictive model

 Examples:

 Notes:
   When linear operators and shapes are given as string expressions the variables a to h can be used as short-hands
   to refer to the number of array elements along respective dimension.

 See also:
   <a href="ml_predictproximal.html" class="code" title="function pred = ml_predictproximal(trials,model)">ml_predictproximal</a>

 References:
  [1] Patrick L. Combettes &amp; Jean-Christophe Pesquet, &quot;Proximal Splitting Methods in Signal Processing&quot;,
      in Fixed-Point Algorithms for Inverse Problems in Science and Engineering, Springer Optimization and Its Applications
      pp. 185-212, 2011

                                Christian Kothe, Swartz Center for Computational Neuroscience, UCSD
                                2013-02-04</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">

<li><a href="ml_calcloss.html" class="code" title="function [measure,stats] = ml_calcloss(type,T,P)">ml_calcloss</a>	Calculate the loss for a set of predictions, given knowledge about the target values.</li>
<li><a href="ml_predictproximal.html" class="code" title="function pred = ml_predictproximal(trials,model)">ml_predictproximal</a>	Prediction function for the proximal framework.</li>
<li><a href="ml_trainproximal.html" class="code" title="function model = ml_trainproximal(varargin)">ml_trainproximal</a>	Learn a linear probabilistic model proximal splitting methods.</li>
<li><a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>	Internal meta-algorithm for voting. Used by other machine learning functions.</li>
</ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">

<li><a href="ml_trainproximal.html" class="code" title="function model = ml_trainproximal(varargin)">ml_trainproximal</a>	Learn a linear probabilistic model proximal splitting methods.</li>
</ul>
<!-- crossreference -->


<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<ul style="list-style-image:url(../../matlabicon.gif)">

<li><a href="#_sub1" class="code">function res = solve_regularization_path(A,y,lambdas,loss,includebias,verbosity,solverOptions,lbfgsOptions,regularizersArg,regweights,weightshape,data_weights)</a></li>
<li><a href="#_sub2" class="code">function [val,grad] = obj_proxlogistic_multitask(x,C,Cp,z,gamma,m,n,data_weights)</a></li>
<li><a href="#_sub3" class="code">function x = prox_logistic_multitask(C,Cp,z,gamma,x0,m,n,args,data_weights)</a></li>
<li><a href="#_sub4" class="code">function obj = obj_logistic_multitask(C,x,m,n,data_weights)</a></li>
<li><a href="#_sub5" class="code">function x = prox_squared_factored_multitask(A,Atb,L,U,rho,z,u,m,n,data_weights)</a></li>
<li><a href="#_sub6" class="code">function obj = obj_squared_multitask(A,b,x,m,n,data_weights)</a></li>
<li><a href="#_sub7" class="code">function [val,grad] = obj_proxlogistic(x,C,Cp,z,gamma,m)</a></li>
<li><a href="#_sub8" class="code">function x = prox_logistic(C,Cp,z,gamma,x0,m,args)</a></li>
<li><a href="#_sub9" class="code">function obj = obj_logistic(C,x,m)</a></li>
<li><a href="#_sub10" class="code">function x = prox_squared_factored(A,Atb,L,U,rho,z,u,m,n)</a></li>
<li><a href="#_sub11" class="code">function x = prox_squared_iter(A,b,rho,z,u,n,x0)</a></li>
<li><a href="#_sub12" class="code">function obj = obj_squared(A,b,x)</a></li>
<li><a href="#_sub13" class="code">function x = prox_l1_simple(z, gamma)</a></li>
<li><a href="#_sub14" class="code">function x = prox_l12_simple(z, gamma, shape)</a></li>
<li><a href="#_sub15" class="code">function x = prox_l2_simple(z, gamma)</a></li>
<li><a href="#_sub16" class="code">function x = prox_l1inf_simple(z, gamma, shape)</a></li>
<li><a href="#_sub17" class="code">function x = prox_nuclear_simple(z, gamma, shape)</a></li>
<li><a href="#_sub18" class="code">function n = norm_l12_simple(z, shape)</a></li>
<li><a href="#_sub19" class="code">function n = norm_l1inf_simple(z, shape)</a></li>
<li><a href="#_sub20" class="code">function n = norm_nuclear_simple(z, shape)</a></li>
<li><a href="#_sub21" class="code">function [val,grad] = obj_proxhs(x,b,z,gamma)</a></li>
<li><a href="#_sub22" class="code">function x = prox_hs(b,z,gamma,x0,args)</a></li>
<li><a href="#_sub23" class="code">function obj = obj_hs(x,b)</a></li>
<li><a href="#_sub24" class="code">function [L,U] = factor(A, rho)</a></li>
</ul>




<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function model = ml_trainproximal(varargin)</a>
0002 <span class="comment">% Learn a linear probabilistic model proximal splitting methods.</span>
0003 <span class="comment">% Model = ml_trainproximal(Trials, Targets, Lambdas, Options...)</span>
0004 <span class="comment">%</span>
0005 <span class="comment">% This function allows to implement linear or logistic regression using a variety of regularization</span>
0006 <span class="comment">% terms and combinations thereof using proximal splitting [1].</span>
0007 <span class="comment">%</span>
0008 <span class="comment">% In:</span>
0009 <span class="comment">%   Trials       : training data matrix, as in ml_train</span>
0010 <span class="comment">%</span>
0011 <span class="comment">%   Targets      : 1d target variable vector, as in ml_train</span>
0012 <span class="comment">%</span>
0013 <span class="comment">%   LossType     : loss function to be used,</span>
0014 <span class="comment">%                  'logistic' for classification (default)</span>
0015 <span class="comment">%                  'squared' for regression</span>
0016 <span class="comment">%                  'hyperbolic-secant' special-purpose for super-Gaussian estimation</span>
0017 <span class="comment">%</span>
0018 <span class="comment">%   Regularizers : Definition of the regularization terms. Any combination of terms is permitted.</span>
0019 <span class="comment">%</span>
0020 <span class="comment">%</span>
0021 <span class="comment">%   Options  : optional name-value parameters to control the training details:</span>
0022 <span class="comment">%</span>
0023 <span class="comment">%               'regweights' : Weights of the regularizers. This is a vector of (relative) regularization</span>
0024 <span class="comment">%                              parameters. If [] set to 1/N for N regularizers. (default: [])</span>
0025 <span class="comment">%                              Can also be a cell array of (normalized) weight vectors; in this case</span>
0026 <span class="comment">%                              it it simultaneously optimized together with the lambdas.</span>
0027 <span class="comment">%</span>
0028 <span class="comment">%               'solverOptions' : cell array of name-value pairs to control how the outer ADMM solver</span>
0029 <span class="comment">%                                 behaves</span>
0030 <span class="comment">%</span>
0031 <span class="comment">%                    'abs_tol' : Absolute tolerance criterion. (default: 10e-4)</span>
0032 <span class="comment">%</span>
0033 <span class="comment">%                    'rel_tol' : Relative tolerance criterion. (default: 10e-3)</span>
0034 <span class="comment">%</span>
0035 <span class="comment">%                    'maxit' : Maximum number of iterations. (default: 1000)</span>
0036 <span class="comment">%</span>
0037 <span class="comment">%                    'rho' : Initial coupling parameter. For proximal algorithms this is the coupling strength</span>
0038 <span class="comment">%                            between the terms between updates. Increasing this can improve the convergence</span>
0039 <span class="comment">%                            speed but too strong values can prevent any convergence. (default: 1)</span>
0040 <span class="comment">%</span>
0041 <span class="comment">%                    'rho_update' : Update Rho. Whether to update rho dynamically according to 3.4.1 in [2].</span>
0042 <span class="comment">%                                   Note, this can sometimes cause r_norm, s_norm to &quot;blow up&quot;. (default: true)</span>
0043 <span class="comment">%</span>
0044 <span class="comment">%                    'rho_cutoff' : Rho update threshold. (default: 10)</span>
0045 <span class="comment">%</span>
0046 <span class="comment">%                    'rho_incr' : Rho update increment factor. (default: 2)</span>
0047 <span class="comment">%</span>
0048 <span class="comment">%                    'rho_decr' : Rho update decrement factor. (default: 2)</span>
0049 <span class="comment">%</span>
0050 <span class="comment">%               'lbfgsOptions' : cell array of name-value pairs to control how the inner LFBGS solver</span>
0051 <span class="comment">%                                behaves</span>
0052 <span class="comment">%</span>
0053 <span class="comment">%                    'm' : LBFGS history length. The number of corrections to approximate the inverse</span>
0054 <span class="comment">%                          hessian matrix. (default: 6)</span>
0055 <span class="comment">%</span>
0056 <span class="comment">%                    'epsilon' : Tolerance criterion.  A minimization terminates when ||g|| &lt; epsilon*max(1,||x||).</span>
0057 <span class="comment">%                                (default: 1e-3)</span>
0058 <span class="comment">%</span>
0059 <span class="comment">%                    'past' : Distance for delta-based convergence test. (default: 0)</span>
0060 <span class="comment">%</span>
0061 <span class="comment">%                    'delta' : Delta for convergence test. (default: 1e-5)</span>
0062 <span class="comment">%</span>
0063 <span class="comment">%                    'MaxIter' : Maximum number of iterations. (default: 10)</span>
0064 <span class="comment">%</span>
0065 <span class="comment">%                    'linesearch' : The line search algorithm. Can be any of the following:</span>
0066 <span class="comment">%                                   {'more_thuente','backtracking_armijo','backtracking_wolfe','backtracking_strong_wolfe'}</span>
0067 <span class="comment">%                                   (default: more_thuente)</span>
0068 <span class="comment">%</span>
0069 <span class="comment">%                    'max_linesearch' : Maximum number of trials for the line search. (default: 40)</span>
0070 <span class="comment">%</span>
0071 <span class="comment">%                    'min_step' : Minimum step of the line search. (default: 1e-20)</span>
0072 <span class="comment">%</span>
0073 <span class="comment">%                    'max_step': Maximum step of the line search routine. (default: 1e20)</span>
0074 <span class="comment">%</span>
0075 <span class="comment">%                    'ftol' : Line search tolerance F. A parameter to control the accuracy of the</span>
0076 <span class="comment">%                             line search routine. (default: 1e-4)</span>
0077 <span class="comment">%</span>
0078 <span class="comment">%                    'wolfe' : Coefficient for the Wolfe condition. (default: 0.9)</span>
0079 <span class="comment">%</span>
0080 <span class="comment">%                    'gtol' : Line search tolerance G. A parameter to control the accuracy of the</span>
0081 <span class="comment">%                             line search routine. (default: 0.9)</span>
0082 <span class="comment">%</span>
0083 <span class="comment">%                    'xtol' : Machine precision for floating-point values. (default: 1e-16)</span>
0084 <span class="comment">%</span>
0085 <span class="comment">%                    'DerivativeCheck' : Derivative check using finite differences. (default: 'off')</span>
0086 <span class="comment">%</span>
0087 <span class="comment">%                    'Display' : Options for displaying progress. (default: 'none')</span>
0088 <span class="comment">%</span>
0089 <span class="comment">%               'lambdaSearch' : cell array of name-value pairs governing the regularization path search</span>
0090 <span class="comment">%</span>
0091 <span class="comment">%                   'lambdas' : Regulariation parameters. Controls the sparsity/simplicity of the result.</span>
0092 <span class="comment">%                               Typically, this is an interval to scan. (default: 2.^(3:-0.25:-5))</span>
0093 <span class="comment">%</span>
0094 <span class="comment">%                   'nfolds' : Cross-validation folds. The cross-validation is used to determine the best</span>
0095 <span class="comment">%                              regularization parameter value (default: 5)</span>
0096 <span class="comment">%</span>
0097 <span class="comment">%                   'foldmargin' : Margin (in trials) between folds. This is the number of trials omitted</span>
0098 <span class="comment">%                                  between training and test sets. (default: 5)</span>
0099 <span class="comment">%</span>
0100 <span class="comment">%                   'cvmetric' : metric to use for parameter optimization; can be any of those supported by</span>
0101 <span class="comment">%                                ml_calcloss. In particular, 'auc' is a good idea if the classification</span>
0102 <span class="comment">%                                task is between highly imbalanced classes. (default: '' = auto-determine)</span>
0103 <span class="comment">%</span>
0104 <span class="comment">%                   'return_regpath' : Return the entire regularization path. If false, only the best model will</span>
0105 <span class="comment">%                                      be returned. (default: true)</span>
0106 <span class="comment">%</span>
0107 <span class="comment">%</span>
0108 <span class="comment">%               'scaling': pre-scaling of the data (see hlp_findscaling for options) (default: 'std')</span>
0109 <span class="comment">%</span>
0110 <span class="comment">%               'data_weights': dataset weights; optional vector of weights for each task in the</span>
0111 <span class="comment">%                               training data (one element per task in a multi-task learning setting) (default: [])</span>
0112 <span class="comment">%</span>
0113 <span class="comment">%               'includebias': whether to include a bias param (default: true)</span>
0114 <span class="comment">%</span>
0115 <span class="comment">%               'verbosity': verbosity level, 0-3 (0=no output)</span>
0116 <span class="comment">%</span>
0117 <span class="comment">% Out:</span>
0118 <span class="comment">%   Models   : a predictive model</span>
0119 <span class="comment">%</span>
0120 <span class="comment">% Examples:</span>
0121 <span class="comment">%</span>
0122 <span class="comment">% Notes:</span>
0123 <span class="comment">%   When linear operators and shapes are given as string expressions the variables a to h can be used as short-hands</span>
0124 <span class="comment">%   to refer to the number of array elements along respective dimension.</span>
0125 <span class="comment">%</span>
0126 <span class="comment">% See also:</span>
0127 <span class="comment">%   ml_predictproximal</span>
0128 <span class="comment">%</span>
0129 <span class="comment">% References:</span>
0130 <span class="comment">%  [1] Patrick L. Combettes &amp; Jean-Christophe Pesquet, &quot;Proximal Splitting Methods in Signal Processing&quot;,</span>
0131 <span class="comment">%      in Fixed-Point Algorithms for Inverse Problems in Science and Engineering, Springer Optimization and Its Applications</span>
0132 <span class="comment">%      pp. 185-212, 2011</span>
0133 <span class="comment">%</span>
0134 <span class="comment">%                                Christian Kothe, Swartz Center for Computational Neuroscience, UCSD</span>
0135 <span class="comment">%                                2013-02-04</span>
0136 
0137 <span class="comment">% definition of regularization terms</span>
0138 regularizer_params = @(name) arg_subswitch({lower(name),name},{<span class="string">'none'</span>},{ <span class="keyword">...</span>
0139     <span class="string">'none'</span>, {}, <span class="keyword">...</span>
0140     <span class="string">'l1'</span>, { <span class="keyword">...</span>
0141     arg({<span class="string">'A'</span>,<span class="string">'LinearOperator'</span>},<span class="string">'@(x)x'</span>,[],<span class="string">'Linear transform. The norm applies to the linearly transformed feature vector. Either an expression that is evaluated in the workspace or a function handle. When defining the linear operator as an anonymous function, the variables a to h can be used to refer to the sizes of the first 8 dimensions of x.'</span>), <span class="keyword">...</span>
0142     arg({<span class="string">'weights'</span>,<span class="string">'FeatureWeights'</span>},[],[],<span class="string">'Weights on the features. Allows for a reweighted the norm (e.g., to impose certain types of priors).'</span>), <span class="keyword">...</span>
0143     }, <span class="keyword">...</span>
0144     <span class="string">'l2'</span>, { <span class="keyword">...</span>
0145     arg({<span class="string">'A'</span>,<span class="string">'LinearOperator'</span>},<span class="string">'@(x)x'</span>,[],<span class="string">'Linear transform. The norm applies to the linearly transformed feature vector. Either an expression that is evaluated in the workspace or a function handle. When defining the linear operator as an anonymous function, the variables a to h can be used to refer to the sizes of the first 8 dimensions of x.'</span>), <span class="keyword">...</span>
0146     arg({<span class="string">'nonorthogonal_transform'</span>,<span class="string">'NonorthogonalTransform'</span>},false,[],<span class="string">'Linear operator is non-orthogonal. In this case an iterative method will be used that is faster and numerically more robust than letting ADMM do it.'</span>), <span class="keyword">...</span>
0147     arg({<span class="string">'y'</span>,<span class="string">'TargetValues'</span>},[],[],<span class="string">'Recenter the norm around target values. This allows for regression problems as side assumptions.'</span>), <span class="keyword">...</span>
0148     arg({<span class="string">'weights'</span>,<span class="string">'FeatureWeights'</span>},[],[],<span class="string">'Weights on the features. Allows for a reweighted norm (e.g., to impose certain types of priors).'</span>), <span class="keyword">...</span>
0149     }, <span class="keyword">...</span>
0150     <span class="string">'l1/l2'</span>, { <span class="keyword">...</span>
0151     arg({<span class="string">'A'</span>,<span class="string">'LinearOperator'</span>},<span class="string">'@(x)x'</span>,[],<span class="string">'Linear transform. The norm applies to the linearly transformed feature vector. Either an expression that is evaluated in the workspace or a function handle. When defining the linear operator as an anonymous function, the variables a to h can be used to refer to the sizes of the first 8 dimensions of x.'</span>), <span class="keyword">...</span>
0152     arg({<span class="string">'g_d'</span>,<span class="string">'GroupIndices'</span>},uint32([]),[],<span class="string">'Feature group indices. This is a vector of indices that form the support of all groups; can also be a matrix. If empty, this defaults to columnwise sparsity.'</span>), <span class="keyword">...</span>
0153     arg({<span class="string">'g_t'</span>,<span class="string">'GroupSizes'</span>},uint32([]),[],<span class="string">'Feature group sizes. This is a vector of successive range lengths on the indices.'</span>), <span class="keyword">...</span>
0154     arg({<span class="string">'weights'</span>,<span class="string">'FeatureWeights'</span>},[],[],<span class="string">'Weights on the features. Allows for a reweighted norm (e.g., to impose certain types of priors).'</span>), <span class="keyword">...</span>
0155     arg({<span class="string">'weights1'</span>,<span class="string">'GroupWeights'</span>},[],[],<span class="string">'Weights on the groups. Allows for a reweighted the norm (e.g., to impose certain types of priors).'</span>) <span class="keyword">...</span>
0156     }, <span class="keyword">...</span>
0157     <span class="string">'l1/linf'</span>, { <span class="keyword">...</span>
0158     arg({<span class="string">'A'</span>,<span class="string">'LinearOperator'</span>},<span class="string">'@(x)x'</span>,[],<span class="string">'Linear transform. The norm applies to the linearly transformed feature vector. Either an expression that is evaluated in the workspace or a function handle. When defining the linear operator as an anonymous function, the variables a to h can be used to refer to the sizes of the first 8 dimensions of x.'</span>), <span class="keyword">...</span>
0159     arg({<span class="string">'g_d'</span>,<span class="string">'GroupIndices'</span>},uint32([]),[],<span class="string">'Feature group indices. This is a vector of indices that form the support of all groups; can also be a matrix. If empty, this defaults to columnwise sparsity.'</span>), <span class="keyword">...</span>
0160     arg({<span class="string">'g_t'</span>,<span class="string">'GroupSizes'</span>},uint32([]),[],<span class="string">'Feature group sizes. This is a vector of successive range lengths on the indices.'</span>), <span class="keyword">...</span>
0161     arg({<span class="string">'weights'</span>,<span class="string">'FeatureWeights'</span>},[],[],<span class="string">'Weights on the features. Allows for a reweighted norm (e.g., to impose certain types of priors).'</span>), <span class="keyword">...</span>
0162     arg({<span class="string">'weights1'</span>,<span class="string">'GroupWeights'</span>},[],[],<span class="string">'Weights on the groups. Allows for a reweighted the norm (e.g., to impose certain types of priors).'</span>) <span class="keyword">...</span>
0163     }, <span class="keyword">...</span>
0164     <span class="string">'tv2d'</span>, { <span class="keyword">...</span>
0165     arg({<span class="string">'A'</span>,<span class="string">'LinearOperator'</span>},<span class="string">'@(x)x'</span>,[],<span class="string">'Linear transform. The norm applies to the linearly transformed feature vector. Either an expression that is evaluated in the workspace or a function handle. When defining the linear operator as an anonymous function, the variables a to h can be used to refer to the sizes of the first 8 dimensions of x.'</span>), <span class="keyword">...</span>
0166     arg({<span class="string">'shape'</span>,<span class="string">'Shape'</span>},<span class="string">''</span>,[],<span class="string">'Final feature shape. Allows to reshape the linearly transformed features into a matrix to apply matrix norms. If empty defaults to the shape of the original features.'</span>,<span class="string">'shape'</span>,<span class="string">'row'</span>), <span class="keyword">...</span>
0167     arg({<span class="string">'useGPU'</span>,<span class="string">'UseGPU'</span>},false,[],<span class="string">'Use GPU acceleration. This is experimental and requires that UnLocBox is started with GPU support enabled.'</span>), <span class="keyword">...</span>
0168     }, <span class="keyword">...</span>
0169     <span class="string">'tv3d'</span>, { <span class="keyword">...</span>
0170     arg({<span class="string">'A'</span>,<span class="string">'LinearOperator'</span>},<span class="string">'@(x)x'</span>,[],<span class="string">'Linear transform. The norm applies to the linearly transformed feature vector. Either an expression that is evaluated in the workspace or a function handle. When defining the linear operator as an anonymous function, the variables a to h can be used to refer to the sizes of the first 8 dimensions of x.'</span>), <span class="keyword">...</span>
0171     arg({<span class="string">'shape'</span>,<span class="string">'Shape'</span>},<span class="string">''</span>,[],<span class="string">'Final feature shape. Allows to reshape the linearly transformed features into a matrix to apply matrix norms. If empty defaults to the shape of the original features.'</span>,<span class="string">'shape'</span>,<span class="string">'row'</span>), <span class="keyword">...</span>
0172     arg({<span class="string">'useGPU'</span>,<span class="string">'UseGPU'</span>},false,[],<span class="string">'Use GPU acceleration. This is experimental and requires that UnLocBox is started with GPU support enabled.'</span>), <span class="keyword">...</span>
0173     }, <span class="keyword">...</span>
0174     <span class="string">'trace'</span>, { <span class="keyword">...</span>
0175     arg({<span class="string">'A'</span>,<span class="string">'LinearOperator'</span>},<span class="string">'@(x)x'</span>,[],<span class="string">'Linear transform. The norm applies to the linearly transformed feature vector. Either an expression that is evaluated in the workspace or a function handle. When defining the linear operator as an anonymous function, the variables a to h can be used to refer to the sizes of the first 8 dimensions of x.'</span>), <span class="keyword">...</span>
0176     arg({<span class="string">'shape'</span>,<span class="string">'Shape'</span>},<span class="string">''</span>,[],<span class="string">'Final feature shape. Allows to reshape the linearly transformed features into a matrix to apply matrix norms. If empty defaults to the shape of the original features.'</span>,<span class="string">'shape'</span>,<span class="string">'row'</span>), <span class="keyword">...</span>
0177     }}, <span class="string">'Regularization term. Defines a term in the optimization problem; multiple types are supported and can be mixed freely.'</span>);
0178 
0179 expose_handles(@<a href="#_sub1" class="code" title="subfunction res = solve_regularization_path(A,y,lambdas,loss,includebias,verbosity,solverOptions,lbfgsOptions,regularizersArg,regweights,weightshape,data_weights)">solve_regularization_path</a>,varargin{:});
0180 
0181 arg_define([0 2],varargin, <span class="keyword">...</span>
0182     arg_norep(<span class="string">'trials'</span>), <span class="keyword">...</span>
0183     arg_norep(<span class="string">'targets'</span>), <span class="keyword">...</span>
0184     arg({<span class="string">'loss'</span>,<span class="string">'LossType'</span>}, <span class="string">'logistic'</span>, {<span class="string">'logistic'</span>,<span class="string">'squared'</span>}, <span class="string">'Loss function to be used. The logistic loss is suited for classification problems, whereas the squared loss is suited for regression problems.'</span>), <span class="keyword">...</span>
0185     arg_sub({<span class="string">'regularizers'</span>,<span class="string">'Regularizers'</span>},{},{ <span class="keyword">...</span>
0186         regularizer_params(<span class="string">'Term1'</span>), <span class="keyword">...</span>
0187         regularizer_params(<span class="string">'Term2'</span>), <span class="keyword">...</span>
0188         regularizer_params(<span class="string">'Term3'</span>), <span class="keyword">...</span>
0189         regularizer_params(<span class="string">'Term4'</span>), <span class="keyword">...</span>
0190         regularizer_params(<span class="string">'Term5'</span>), <span class="keyword">...</span>
0191         regularizer_params(<span class="string">'Term6'</span>), <span class="keyword">...</span>
0192         regularizer_params(<span class="string">'Term7'</span>)}, <span class="string">'Definition of the regularization terms. Any combination of terms is permitted.'</span>), <span class="keyword">...</span>
0193     arg({<span class="string">'regweights'</span>,<span class="string">'TermWeights'</span>},{[]},[],<span class="string">'Weights of the regularizers. This is a cell array of vectors of (relative) regularization parameters. Default is 1/N for N regularization terms. The cell array lists all possible assignments to search over.'</span>,<span class="string">'type'</span>,<span class="string">'expression'</span>,<span class="string">'shape'</span>,<span class="string">'row'</span>), <span class="keyword">...</span>
0194     arg_sub({<span class="string">'solverOptions'</span>,<span class="string">'SolverOptions'</span>},{},{ <span class="keyword">...</span>
0195         arg({<span class="string">'maxit'</span>,<span class="string">'MaxIterations'</span>},2000,uint32([1 100 5000 10000]),<span class="string">'Maximum number of iterations.'</span>), <span class="keyword">...</span>
0196         arg({<span class="string">'rel_tol'</span>,<span class="string">'RelativeTolerance'</span>},1e-3,[0 1],<span class="string">'Relative tolerance criterion. If the relative difference between two successive iterates is lower than this value the algorithm terminates.'</span>),<span class="keyword">...</span>
0197         arg({<span class="string">'abs_tol'</span>,<span class="string">'AbsoluteTolerance'</span>},0.000001,[0 Inf],<span class="string">'Absolute tolerance criterion. If the objective function value falls below this the algorithm terminates.'</span>), <span class="keyword">...</span>
0198         arg({<span class="string">'rho'</span>,<span class="string">'CouplingParameter'</span>},4,[0 1 30 Inf],<span class="string">'Initial coupling parameter. For proximal algorithms this is the coupling strength between the terms between updates. Increasing this can improve the convergence speed but too strong values can prevent any convergence.'</span>), <span class="keyword">...</span>
0199         arg({<span class="string">'rho_update'</span>,<span class="string">'RhoUpdate'</span>},true,[],<span class="string">'Update Rho. Whether to update rho dynamically according to 3.4.1 in [1]. Note, this can sometimes cause r_norm, s_norm to &quot;blow up&quot;.'</span>), <span class="keyword">...</span>
0200         arg({<span class="string">'rho_cutoff'</span>,<span class="string">'RhoUpdateThreshold'</span>},10.0,[0 2 20 Inf],<span class="string">'Rho update threshold.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0201         arg({<span class="string">'rho_incr'</span>,<span class="string">'RhoUpdateIncr'</span>},2.0,[1 1.5 3 Inf],<span class="string">'Rho update increment factor.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0202         arg({<span class="string">'rho_decr'</span>,<span class="string">'RhoUpdateDecr'</span>},2.0,[1 1.5 3 Inf],<span class="string">'Rho update decrement factor.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0203         arg({<span class="string">'warmstart'</span>,<span class="string">'Warmstart'</span>},true,[],<span class="string">'Warm-start through regularization path. Enabling this is more efficient but convergence issues can be harder to trace down.'</span>) <span class="keyword">...</span>
0204     }, <span class="string">'Controls the behavior of the ADMM optimization algorithm.'</span>), <span class="keyword">...</span>
0205     arg_sub({<span class="string">'lbfgsOptions'</span>,<span class="string">'LBFGSOptions'</span>},{},{ <span class="keyword">...</span>
0206         arg({<span class="string">'MaxIter'</span>,<span class="string">'MaxIterations'</span>},10,uint32([1 1 20 1000]),<span class="string">'Maximum number of iterations.'</span>), <span class="keyword">...</span>
0207         arg({<span class="string">'m'</span>,<span class="string">'HessianHistory'</span>},6,uint32([1 4 10 20]),<span class="string">'LBFGS history length. The number of corrections to approximate the inverse hessian matrix.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0208         arg({<span class="string">'epsilon'</span>,<span class="string">'Epsilon'</span>},1e-3,[],<span class="string">'Tolerance criterion.  A minimization terminates when ||g|| &lt; epsilon*max(1,||x||).'</span>), <span class="keyword">...</span>
0209         arg({<span class="string">'past'</span>,<span class="string">'DeltaDistance'</span>},0,[],<span class="string">'Distance for delta-based convergence test.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0210         arg({<span class="string">'delta'</span>,<span class="string">'Delta'</span>},1e-5,[],<span class="string">'Delta for convergence test.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0211         arg({<span class="string">'linesearch'</span>,<span class="string">'LineSearchAlgorithm'</span>},<span class="string">'more_thuente'</span>,{<span class="string">'more_thuente'</span>,<span class="string">'backtracking_armijo'</span>,<span class="string">'backtracking_wolfe'</span>,<span class="string">'backtracking_strong_wolfe'</span>},<span class="string">'The line search algorithm.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0212         arg({<span class="string">'max_linesearch'</span>,<span class="string">'MaxLineSearch'</span>},40,uint32([0 10 100 1000]),<span class="string">'Maximum number of trials for the line search.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0213         arg({<span class="string">'min_step'</span>,<span class="string">'MinStepsize'</span>},1e-20,[],<span class="string">' Minimum step of the line search.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0214         arg({<span class="string">'max_step'</span>,<span class="string">'MaxStepsize'</span>},1e20,[],<span class="string">' Maximum step of the line search routine.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0215         arg({<span class="string">'ftol'</span>,<span class="string">'FTolerance'</span>},1e-4,[],<span class="string">'Line search tolerance F. A parameter to control the accuracy of the line search routine.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0216         arg({<span class="string">'wolfe'</span>,<span class="string">'WolfeCoefficient'</span>}, 0.9,[],<span class="string">'Coefficient for the Wolfe condition.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0217         arg({<span class="string">'gtol'</span>,<span class="string">'GTolerance'</span>},0.9,[],<span class="string">'Line search tolerance G. A parameter to control the accuracy of the line search routine.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0218         arg({<span class="string">'xtol'</span>,<span class="string">'XTolerance'</span>},1e-16,[],<span class="string">'Machine precision for floating-point values.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0219         arg({<span class="string">'useGPU'</span>,<span class="string">'UseGPU'</span>},true,[],<span class="string">'Run on the GPU if possible.'</span>), <span class="keyword">...</span>
0220         arg(<span class="string">'DerivativeCheck'</span>,<span class="string">'off'</span>,{<span class="string">'off'</span>,<span class="string">'on'</span>},<span class="string">' Derivative check using finite differences.'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0221         arg({<span class="string">'Display'</span>,<span class="string">'Verbosity'</span>},<span class="string">'none'</span>,{<span class="string">'none'</span>,<span class="string">'on'</span>},<span class="string">'Options for displaying progress.'</span>) <span class="keyword">...</span>
0222     },<span class="string">'Options of the inner LBFGS solver. This is for the logistic objective function.'</span>), <span class="keyword">...</span>
0223     arg_sub({<span class="string">'lambdaSearch'</span>,<span class="string">'LambdaSearch'</span>},{},{ <span class="keyword">...</span>
0224         arg({<span class="string">'lambdas'</span>,<span class="string">'Lambdas'</span>}, 2.^(3:-0.66:-8), [0 2^-8 2^15 Inf], <span class="string">'Regulariation parameters. Controls the sparsity/simplicity of the result. Typically, this is an interval to scan, such as 2.^(10:-1:-15).'</span>), <span class="keyword">...</span>
0225         arg({<span class="string">'nfolds'</span>,<span class="string">'NumFolds'</span>},5,[],<span class="string">'Cross-validation folds. The cross-validation is used to determine the best regularization parameter.'</span>,<span class="string">'shape'</span>,<span class="string">'row'</span>),<span class="keyword">...</span>
0226         arg({<span class="string">'force_cv'</span>,<span class="string">'ForceCV'</span>},false,[],<span class="string">'Force cross-validation. This will perform a nested cross-validation even if there is only one lambda/regweight (i.e., a search wouldn''t be strictly necessary). Can be useful to simultaneously run multiple within-task/subject cross-validations given fixed reg parameters.'</span>,<span class="string">'shape'</span>,<span class="string">'row'</span>),<span class="keyword">...</span>
0227         arg({<span class="string">'foldmargin'</span>,<span class="string">'FoldMargin'</span>},0,uint32([0 0 10 1000]),<span class="string">'Margin between folds. This is the number of trials omitted between training and test set.'</span>), <span class="keyword">...</span>
0228         arg({<span class="string">'cvmetric'</span>,<span class="string">'ParameterMetric'</span>},<span class="string">''</span>,{<span class="string">''</span>,<span class="string">'kld'</span>,<span class="string">'nll'</span>,<span class="string">'mcr'</span>,<span class="string">'mae'</span>,<span class="string">'mse'</span>,<span class="string">'max'</span>,<span class="string">'rms'</span>,<span class="string">'bias'</span>,<span class="string">'medse'</span>,<span class="string">'auc'</span>,<span class="string">'cond_entropy'</span>,<span class="string">'cross_entropy'</span>,<span class="string">'f_measure'</span>},<span class="string">'Metric for Parameter Optimization. By default auto-determined; can be any of the ml_calcloss-supported metrics. In particular, auc is a good idea if the classification task is between highly imbalanced classes.'</span>) <span class="keyword">...</span>
0229         arg({<span class="string">'return_regpath'</span>,<span class="string">'ReturnRegpath'</span>}, true, [], <span class="string">'Return the entire regularization path. This is for the best relative weighting of terms. If false, only the best model will be returned.'</span>), <span class="keyword">...</span>
0230         arg({<span class="string">'return_reggrid'</span>,<span class="string">'ReturnReggrid'</span>}, false, [], <span class="string">'Return the entire regularization grid. This also returns regularization paths for all other relative weightings. Warning: this can require a lot of memory (depending on model size).'</span>), <span class="keyword">...</span>
0231         arg({<span class="string">'history_traces'</span>,<span class="string">'HistoryTraces'</span>}, false, [], <span class="string">'Return history traces. If true, optimization history traces will be returned. Warning: this will require a very large amount of memory (depending on model size).'</span>), <span class="keyword">...</span>
0232     }, <span class="string">'Controls the search for the optimal regularization parameter.'</span>), <span class="keyword">...</span>
0233     arg({<span class="string">'scaling'</span>,<span class="string">'Scaling'</span>}, <span class="string">'std'</span>, {<span class="string">'none'</span>,<span class="string">'center'</span>,<span class="string">'std'</span>,<span class="string">'minmax'</span>,<span class="string">'whiten'</span>}, <span class="string">'Pre-scaling of the data. For the regulariation to work best, the features should either be naturally scaled well, or be artificially scaled.'</span>), <span class="keyword">...</span>
0234     arg_nogui({<span class="string">'shape'</span>,<span class="string">'Shape'</span>}, [], [], <span class="string">'Reshaping for features. Allows to reshape (perhaps vectorized) features into a particular representation.'</span>,<span class="string">'shape'</span>,<span class="string">'row'</span>), <span class="keyword">...</span>
0235     arg({<span class="string">'data_weights'</span>,<span class="string">'DataWeights'</span>}, [], [], <span class="string">'Dataset weights. Optional vector of weights for each task in the training data (one element per task in a multi-task learning setting).'</span>), <span class="keyword">...</span>
0236     arg({<span class="string">'verbosity'</span>,<span class="string">'Verbosity'</span>},1,uint32([1 5]),<span class="string">'Diagnostic output level. Zero is off, 1 only shows cross-validation diagnostics, 2 shows solver diagnostics, 3 shows iteration diagnostics.'</span>), <span class="keyword">...</span>
0237     arg({<span class="string">'continuous_targets'</span>,<span class="string">'ContinuousTargets'</span>,<span class="string">'Regression'</span>}, false, [], <span class="string">'Whether to use continuous targets. This allows to implement some kind of damped regression approach.'</span>),<span class="keyword">...</span>
0238     arg({<span class="string">'votingScheme'</span>,<span class="string">'VotingScheme'</span>},<span class="string">'1v1'</span>,{<span class="string">'1v1'</span>,<span class="string">'1vR'</span>},<span class="string">'Voting scheme. If multi-class classification is used, this determine how binary classifiers are arranged to solve the multi-class problem. 1v1 gets slow for large numbers of classes (as all pairs are tested), but can be more accurate than 1vR.'</span>), <span class="keyword">...</span>
0239     arg({<span class="string">'parallel_scope'</span>,<span class="string">'ParallelScope'</span>},[],[],<span class="string">'Optional parallel scope. If this is a cell array of name-value pairs, cluster resources will be acquired with these options for the duration of bci_train (and released thereafter) Options as in env_acquire_cluster.'</span>,<span class="string">'type'</span>,<span class="string">'expression'</span>), <span class="keyword">...</span>
0240     arg({<span class="string">'engine_cv'</span>,<span class="string">'ParallelEngine'</span>,<span class="string">'engine'</span>},<span class="string">'global'</span>,{<span class="string">'global'</span>,<span class="string">'local'</span>,<span class="string">'BLS'</span>,<span class="string">'Reference'</span>,<span class="string">'ParallelComputingToolbox'</span>}, <span class="string">'Parallel engine to use. This can either be one of the supported parallel engines (BLS for BCILAB Scheduler, Reference for a local reference implementation, and ParallelComputingToolbox for a PCT-based implementation), or local to skip parallelization altogether, or global to select the currently globally selected setting (in the global tracking variable).'</span>), <span class="keyword">...</span>
0241     arg({<span class="string">'includebias'</span>,<span class="string">'IncludeBias'</span>,<span class="string">'bias'</span>},true,[],<span class="string">'Include bias param. Also learns an unregularized bias param (strongly recommended for typical classification problems).'</span>));
0242 
0243 <span class="keyword">if</span> ~iscell(targets)
0244     trials = {trials};
0245     targets = {targets}; 
0246 <span class="keyword">end</span>
0247 
0248 <span class="keyword">for</span> t=1:length(trials)
0249     trials{t} = real(trials{t}); <span class="keyword">end</span>
0250 
0251 <span class="comment">% find all target classes (if classification)</span>
0252 nTasks = length(targets);
0253 classes = unique(vertcat(targets{:}));
0254 <span class="keyword">if</span> length(classes) &gt; 2 &amp;&amp; strcmp(loss,<span class="string">'logistic'</span>) &amp;&amp; ~continuous_targets
0255     <span class="comment">% in the multi-class case we use the voter for now (TODO: use softmax loss instead)</span>
0256     model = <a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>(trials, targets, votingScheme, @<a href="ml_trainproximal.html" class="code" title="function model = ml_trainproximal(varargin)">ml_trainproximal</a>, @<a href="ml_predictproximal.html" class="code" title="function pred = ml_predictproximal(trials,model)">ml_predictproximal</a>, varargin{:});
0257 <span class="keyword">elseif</span> length(classes) == 1
0258     error(<span class="string">'BCILAB:only_one_class'</span>,<span class="string">'Your training data set has no trials for one of your classes; you need at least two classes to train a classifier.\n\nThe most likely reasons are that one of your target markers does not occur in the data, or that all your trials of a particular class are concentrated in a single short segment of your data (10 or 20 percent). The latter would be a problem with the experiment design.'</span>);
0259 <span class="keyword">else</span>
0260         
0261     <span class="keyword">if</span> isscalar(lambdaSearch.nfolds)
0262         <span class="comment">% if nfolds is in [0..1], we take it as a function of #trials</span>
0263     <span class="keyword">if</span> lambdaSearch.nfolds &lt; 1 &amp;&amp; lambdaSearch.nfolds &gt; 0
0264         lambdaSearch.nfolds = round(lambdaSearch.nfolds*mean(cellfun(<span class="string">'length'</span>,targets))); <span class="keyword">end</span>
0265         <span class="comment">% (if instead it's in [-1..0], we take it as the p in p-holdout)</span>
0266         <span class="comment">% (else if an integer, we take it as the number of folds)</span>
0267     nFolds = ceil(abs(lambdaSearch.nfolds));
0268     <span class="keyword">elseif</span> isequal(size(lambdaSearch.nfolds), [1 2])
0269         <span class="comment">% if nfolds is of the form [low high], we treat these numbers as an interval specification,</span>
0270         <span class="comment">% where low and high are taken as fractions of the number of trials</span>
0271         nFolds = 1;
0272     <span class="keyword">else</span>
0273         error(<span class="string">'NumFolds format is unsupported.'</span>);
0274     <span class="keyword">end</span>
0275     
0276     <span class="comment">% sanitize some more inputs</span>
0277     solverOptions.verbose = max(0,verbosity-1);    
0278     <span class="keyword">if</span> isnumeric(regweights)
0279         regweights = {regweights}; <span class="keyword">end</span>
0280     nRegweights = length(regweights);
0281     
0282     <span class="comment">% lambdas need to be sorted in descending order for the warm-starting to work</span>
0283     nLambdas = length(lambdaSearch.lambdas);
0284     lambdaSearch.lambdas = sort(lambdaSearch.lambdas,<span class="string">'ascend'</span>);
0285     <span class="keyword">if</span> strcmp(lambdaSearch.cvmetric,<span class="string">'mcr'</span>)
0286         lambdaSearch.cvmetric = <span class="string">''</span>; <span class="keyword">end</span>
0287     
0288     <span class="comment">% determine featureshape and vectorize data if necessary</span>
0289     [featureshape,trials,vectorize_trials] = utl_determine_featureshape(trials,shape);
0290     weightshape = [featureshape nTasks];
0291     
0292     <span class="comment">% optionally scale the data</span>
0293     sc_info = hlp_findscaling(vertcat(trials{:}),scaling);
0294     trials = cellfun(@(t)hlp_applyscaling(t,hlp_findscaling(t,scaling)),trials,<span class="string">'UniformOutput'</span>,false);
0295     
0296     <span class="comment">% optionally remap target labels to -1,+1</span>
0297     <span class="keyword">if</span> strcmp(loss,<span class="string">'logistic'</span>) &amp;&amp; length(classes) == 2 &amp;&amp; ~continuous_targets
0298         <span class="keyword">for</span> t=1:nTasks
0299             targets{t}(targets{t}==classes(1)) = -1;
0300             targets{t}(targets{t}==classes(2)) = +1;
0301         <span class="keyword">end</span>
0302     <span class="keyword">end</span>
0303     
0304     <span class="comment">% ensure that data_weights exists and is scaled properly (we normalize data_weights to sum to</span>
0305     <span class="comment">% nTasks, since the regularizers will usually also be scaled by nTasks)</span>
0306     <span class="keyword">if</span> isempty(data_weights)
0307         data_weights = ones(1,nTasks); <span class="keyword">end</span>
0308     data_weights = data_weights/sum(data_weights)*nTasks;
0309         
0310     <span class="comment">% learn a sequence of models across the given lambda's, on all the data (i.e. the regularization path)</span>
0311     <span class="keyword">if</span> verbosity
0312         disp(<span class="string">'Running optimization...'</span>); <span class="keyword">end</span>
0313     
0314     
0315     <span class="comment">% run a cross-validation to score the lambdas and regweights</span>
0316 
0317     <span class="comment">% loss_means{regweight,task}(fold,lambda) is the average loss for a given task, regularization weight setting, cross-validation fold, and lambda setting</span>
0318     loss_means = cell(nRegweights,nTasks);    
0319     <span class="comment">% predictions{regweight,task}(trial,lambda) is the classifier prediction for a given task, regweight setting, trial, and lambda choice</span>
0320     predictions = repmat(cellfun(@(t)zeros(length(t),nLambdas),targets(:)',<span class="string">'UniformOutput'</span>,false),nRegweights,1);
0321     <span class="comment">% foldid{task}(trial) is the fold in which a given trial is in the test set, for a given task</span>
0322     <span class="keyword">if</span> isequal(size(lambdaSearch.nfolds), [1 2])
0323         <span class="comment">% interval form</span>
0324         p = lambdaSearch.nfolds;
0325         foldids = cellfun(@(t)(0:length(t)-1)/length(t) &gt; p(1) &amp; (0:length(t)-1)/length(t) &lt; p(2),targets,<span class="string">'UniformOutput'</span>,false);
0326     <span class="keyword">elseif</span> lambdaSearch.nfolds &lt; 0 &amp;&amp; lambdaSearch.nfolds &gt; -1
0327         p = abs(lambdaSearch.nfolds);
0328         <span class="comment">% negative fractional value encodes p-holdout (positive fractional value is already defined</span>
0329         <span class="comment">% as the a fraction of the number of trials)</span>
0330         foldids = cellfun(@(t)(0:length(t)-1)/length(t)&gt;(1-p),targets,<span class="string">'UniformOutput'</span>,false);
0331     <span class="keyword">else</span>
0332         foldids = cellfun(@(t)1+floor((0:length(t)-1)/length(t)*nFolds),targets,<span class="string">'UniformOutput'</span>,false);
0333     <span class="keyword">end</span>
0334     
0335     <span class="keyword">if</span> (nLambdas*nRegweights) &gt; 1 || lambdaSearch.force_cv
0336         <span class="comment">% for each fold...</span>
0337         model_seq = cell(nFolds,nRegweights); <span class="comment">% model_seq(fold,regweight}{lambda}{task} is the model for a given fold, regweight and lambda setting, and task</span>
0338         history_seq = cell(nFolds,nRegweights); <span class="comment">% history_seq(fold,regweight}{lambda}( is a struct of optimization histories for a given fold, regweight and lambda setting, for all concurrent tasks</span>
0339         jobs = {}; <span class="comment">% compute jobs</span>
0340         <span class="keyword">for</span> f = 1:nFolds
0341             <span class="keyword">if</span> verbosity
0342                 disp([<span class="string">'Fitting fold # '</span> num2str(f) <span class="string">' of '</span> num2str(nFolds)]); <span class="keyword">end</span>
0343 
0344             <span class="comment">% determine training and test set masks</span>
0345             <span class="comment">% TODO: calc all this per fold and don't recalc below</span>
0346             testmask{f} = cellfun(@(foldid)foldid==f,foldids,<span class="string">'UniformOutput'</span>,false); <span class="comment">% testmask{fold}{task}(trial) a bitmask of test-set trials for a given task</span>
0347             trainmask{f} = cellfun(@(x)~x,testmask{f},<span class="string">'UniformOutput'</span>,false);           <span class="comment">% trainmask{fold}{task}(trial) is a bitmask of train-set trials</span>
0348             <span class="comment">% cut train/test margins into trainmask</span>
0349             <span class="keyword">for</span> t=1:nTasks
0350                 testpos = find(testmask{f}{t});
0351                 <span class="keyword">for</span> j=1:lambdaSearch.foldmargin
0352                     trainmask{f}{t}(max(1,testpos-j)) = false;
0353                     trainmask{f}{t}(min(length(testmask{f}{t}),testpos+j)) = false;
0354                 <span class="keyword">end</span>
0355             <span class="keyword">end</span>
0356 
0357             <span class="comment">% set up design matrices</span>
0358             [A{f},y{f},B{f},z{f}] = deal(cell(1,nTasks));
0359             <span class="keyword">for</span> t=1:nTasks
0360                 <span class="comment">% training data</span>
0361                 A{f}{t} = trials{t}(trainmask{f}{t},:);
0362                 y{f}{t} = targets{t}(trainmask{f}{t});
0363                 <span class="comment">% test data</span>
0364                 B{f}{t} = [trials{t}(testmask{f}{t},:) ones(nnz(testmask{f}{t}),double(includebias))];
0365                 z{f}{t} = targets{t}(testmask{f}{t});
0366             <span class="keyword">end</span>
0367 
0368             <span class="comment">% for each relative regularization term weighting...</span>
0369             <span class="keyword">for</span> w = 1:nRegweights
0370                 jobs{end+1} = {@hlp_diskcache,<span class="string">'predictivemodels'</span>,@<a href="#_sub1" class="code" title="subfunction res = solve_regularization_path(A,y,lambdas,loss,includebias,verbosity,solverOptions,lbfgsOptions,regularizersArg,regweights,weightshape,data_weights)">solve_regularization_path</a>,A{f},y{f},lambdaSearch.lambdas,loss,includebias,verbosity,solverOptions,lbfgsOptions,regularizers,regweights{w},weightshape,data_weights}; <span class="keyword">end</span>            
0371         <span class="keyword">end</span>
0372         
0373         <span class="comment">% run the jobs</span>
0374         results = par_schedule(jobs, <span class="string">'engine'</span>,engine_cv, <span class="string">'scope'</span>,parallel_scope);
0375         predictions = repmat(cellfun(@(t)zeros(length(t),nLambdas),targets(:)',<span class="string">'UniformOutput'</span>,false),nRegweights,1);
0376         ji = 1;
0377         <span class="keyword">for</span> f = 1:nFolds
0378             <span class="keyword">for</span> w = 1:nRegweights
0379                 [model_seq{f,w},history_seq{f,w}] = deal(results{ji}.regpath,results{ji}.hist); ji = ji+1;
0380                 <span class="comment">% for each task...</span>
0381                 <span class="keyword">for</span> t = 1:nTasks
0382                     <span class="comment">% calc test-set predictions for each model</span>
0383                     <span class="keyword">for</span> m=nLambdas:-1:1
0384                         predictions{w,t}(testmask{f}{t},m) = (B{f}{t}*model_seq{f,w}{m}{t}(:))'; <span class="keyword">end</span>
0385                     <span class="keyword">if</span> strcmp(loss,<span class="string">'logistic'</span>)
0386                         predictions{w,t}(testmask{f}{t},:) = 2*(1 ./ (1 + exp(-predictions{w,t}(testmask{f}{t},:))))-1; <span class="keyword">end</span>
0387 
0388                     <span class="comment">% evaluate test-set losses</span>
0389                     <span class="keyword">if</span> isempty(lambdaSearch.cvmetric)
0390                         <span class="keyword">if</span> strcmp(loss,<span class="string">'logistic'</span>)
0391                             loss_means{w,t}(f,:) = mean(~bsxfun(@eq,z{f}{t},sign(predictions{w,t}(testmask{f}{t},:))));
0392                         <span class="keyword">else</span>
0393                             loss_means{w,t}(f,:) = mean((bsxfun(@minus,z{f}{t},predictions{w,t}(testmask{f}{t},:))).^2);
0394                         <span class="keyword">end</span>
0395                     <span class="keyword">else</span>
0396                         <span class="keyword">for</span> m=1:nLambdas
0397                             loss_means{w,t}(f,m) = <a href="ml_calcloss.html" class="code" title="function [measure,stats] = ml_calcloss(type,T,P)">ml_calcloss</a>(lambdaSearch.cvmetric,z{f}{t},predictions{w,t}(testmask{f}{t},m)); <span class="keyword">end</span>
0398                     <span class="keyword">end</span>
0399                 <span class="keyword">end</span>
0400             <span class="keyword">end</span>
0401         <span class="keyword">end</span>
0402     <span class="keyword">else</span>
0403         <span class="comment">% we skip the nested cross-validation if there is only one lambda and one regweight</span>
0404         disp(<span class="string">'Skipping nested cross-validation (only 1 lambda/regweight)...'</span>); 
0405         model_seq = cell(nFolds,nRegweights); 
0406         history_seq = cell(nFolds,nRegweights);
0407         loss_means = repmat({zeros(nFolds,nLambdas)},[nRegweights,nTasks]);
0408         lambdaSearch.return_regpath = false;
0409     <span class="keyword">end</span>
0410 
0411     <span class="comment">% find best lambdas &amp; regweight combination</span>
0412     [loss_mean,best_lambdas,best_loss] = deal(cell(nRegweights,nTasks));
0413     [best_regidx,best_regweights,best_lambda] = deal(cell(1,nTasks));
0414     <span class="keyword">for</span> t=1:nTasks
0415         <span class="keyword">for</span> w=1:nRegweights
0416             <span class="comment">% average loss over folds</span>
0417             loss_mean{w,t} = mean(loss_means{w,t},1);
0418             <span class="comment">% if there are several minima, choose largest lambda of the smallest cvm</span>
0419             best_lambdas{w,t} = max(lambdaSearch.lambdas(loss_mean{w,t} &lt;= min(loss_mean{w,t})));
0420             best_loss{w,t} = min(loss_mean{w,t});
0421         <span class="keyword">end</span>
0422         <span class="comment">% pick regweights and lambda at best loss</span>
0423         [dummy,best_regidx{t}] = min([best_loss{:,t}]); <span class="comment">%#ok&lt;ASGLU&gt;</span>
0424         best_regweights{t} = regweights{best_regidx{t}}(:)';
0425         best_lambda{t} = best_lambdas{best_regidx{t},t};
0426     <span class="keyword">end</span>
0427     
0428     <span class="comment">% select a lambda and regweights combination from all tasks</span>
0429     tmp = sort([best_lambda{:}]); joint_best_lambda = tmp(round(end/2));
0430     tmp = sort(vertcat(best_regweights{:})); 
0431     <span class="keyword">if</span> isempty(tmp)
0432         joint_best_regweights = [];
0433     <span class="keyword">else</span>
0434         joint_best_regweights = tmp(round(end/2),:);
0435     <span class="keyword">end</span>
0436 
0437     <span class="comment">% pick the model at the minimum...</span>
0438     <span class="keyword">if</span> lambdaSearch.return_regpath
0439         <span class="comment">% run the whole regularization path for the jointly best regweight combination</span>
0440         res = hlp_diskcache(<span class="string">'predictivemodels'</span>,@<a href="#_sub1" class="code" title="subfunction res = solve_regularization_path(A,y,lambdas,loss,includebias,verbosity,solverOptions,lbfgsOptions,regularizersArg,regweights,weightshape,data_weights)">solve_regularization_path</a>,trials,targets,lambdaSearch.lambdas,loss,includebias,verbosity,solverOptions,lbfgsOptions,regularizers,joint_best_regweights,weightshape,data_weights);
0441         [regpath,history] = deal(res.regpath, res.hist);
0442         model.regularization_path = regpath;                                  <span class="comment">% the model for a given {lambda}{task} at best regweights, for whole data</span>
0443         model.regularization_loss = loss_mean(best_regidx,:);                 <span class="comment">% the associated loss estimatses for a given {task}(lambda)</span>
0444         model.ws = regpath{find(lambdaSearch.lambdas == joint_best_lambda,1)};<span class="comment">% the best model for a given {task}</span>
0445     <span class="keyword">else</span>
0446         res = hlp_diskcache(<span class="string">'predictivemodels'</span>,@<a href="#_sub1" class="code" title="subfunction res = solve_regularization_path(A,y,lambdas,loss,includebias,verbosity,solverOptions,lbfgsOptions,regularizersArg,regweights,weightshape,data_weights)">solve_regularization_path</a>,trials,targets,lambdaSearch.lambdas(find(lambdaSearch.lambdas==joint_best_lambda,1)),loss,includebias,verbosity,solverOptions,lbfgsOptions,regularizers,joint_best_regweights,weightshape,data_weights);
0447         [tmp,history] = deal(res.regpath, res.hist);
0448         model.ws = tmp{1};  <span class="comment">% optimal model for each {task}</span>
0449     <span class="keyword">end</span>
0450     
0451     model.w = model.ws;
0452     <span class="keyword">if</span> length(model.w) == 1
0453         model.w = model.w{1}; <span class="keyword">end</span>                   <span class="comment">% optimal model for each {task}, or the model if only one task given (without the enclosing cell array)</span>
0454     
0455     model.balance_losses = loss_mean;               <span class="comment">% same as loss_means, legacy</span>
0456     <span class="keyword">if</span> lambdaSearch.return_reggrid
0457         model.regularization_grid = model_seq; <span class="keyword">end</span>  <span class="comment">% sequence of models for each {fold,regweight}{lambda}{task}</span>
0458     <span class="keyword">if</span> lambdaSearch.history_traces
0459         model.fold_history = history_seq;           <span class="comment">% structure of regpath history for each {fold,regweight}{lambda} -- HUGE!</span>
0460         model.regularization_history = history;     <span class="comment">% structure of regpath history at best lambda/regweights</span>
0461     <span class="keyword">end</span>
0462     model.loss_means = loss_means;                  <span class="comment">% overall loss for each {regweight,task}(fold,lambda)</span>
0463     model.loss_mean = loss_mean;                    <span class="comment">% overall loss for each {regweight,task}(lambda)</span>
0464     model.best_regweights = best_regweights;        <span class="comment">% best regweights for each {task}</span>
0465     model.best_lambdas = best_lambdas;              <span class="comment">% best set of lambdas for each {regidx,task}</span>
0466     model.best_lambda = best_lambda;                <span class="comment">% best lambda for each {task}</span>
0467     model.best_loss = best_loss;                    <span class="comment">% best loss for each {regweight,task}</span>
0468     model.classes = classes;                        <span class="comment">% set of class labels in training data</span>
0469     model.continuous_targets = continuous_targets;  
0470     model.includebias = includebias;                <span class="comment">% whether a bias is included in the model</span>
0471     model.vectorize_trials = vectorize_trials;      <span class="comment">% whether trials need to be vectorized first</span>
0472     model.featureshape = featureshape;              <span class="comment">% shape vector for features</span>
0473     model.sc_info = sc_info;                        <span class="comment">% overall scaling info</span>
0474     model.loss = loss;                              <span class="comment">% loss function name</span>
0475 <span class="keyword">end</span>
0476 
0477 
0478 
0479 <span class="comment">% learn the regularization path</span>
0480 <a name="_sub1" href="#_subfunctions" class="code">function res = solve_regularization_path(A,y,lambdas,loss,includebias,verbosity,solverOptions,lbfgsOptions,regularizersArg,regweights,weightshape,data_weights)</a>
0481 <span class="comment">% solve_regularization_path_version&lt;1.0.3&gt;</span>
0482 <span class="keyword">if</span> ~includebias
0483     error(<span class="string">'This implementation currently requires that a bias is included.'</span>); <span class="keyword">end</span>
0484 nTasks = length(A);
0485 
0486 <span class="comment">% m trials, n features, per task</span>
0487 m = cellfun(<span class="string">'size'</span>,A,1);
0488 n = cellfun(<span class="string">'size'</span>,A,2);
0489 <span class="keyword">if</span> length(unique(n)) &gt; 1
0490     error(<span class="string">'Each task must have the same number of features.'</span>); <span class="keyword">end</span>
0491 
0492 <span class="comment">% w is the concatenation of model weights for all tasks, followed by the unregularized biases for each task</span>
0493 w = zeros(sum(n) + nTasks,1);
0494 
0495 <span class="comment">% set up the design matrix A &amp; label vector y</span>
0496 A = cellfun(@(A)double(A),A,<span class="string">'UniformOutput'</span>,false);
0497 y = cellfun(@(y)double(y(:)),y,<span class="string">'UniformOutput'</span>,false);
0498 
0499 <span class="comment">% set up the data-dependent loss function to use</span>
0500 <span class="keyword">switch</span> loss
0501     <span class="keyword">case</span> <span class="string">'logistic'</span>
0502         C = cellfun(@(A,y)[bsxfun(@times,-y,A) -y],A,y,<span class="string">'UniformOutput'</span>,false);
0503         <span class="keyword">if</span> lbfgsOptions.useGPU
0504             <span class="keyword">try</span>
0505                 C = cellfun(@gpuArray,C,<span class="string">'UniformOutput'</span>,false);
0506             <span class="keyword">catch</span> e
0507                 disp_once([<span class="string">'Could not enable GPU support: '</span> e.message]);
0508             <span class="keyword">end</span>
0509         <span class="keyword">end</span>
0510         Cp = cellfun(@transpose,C,<span class="string">'UniformOutput'</span>,false);
0511         lossfunc.prox = @(x,gamma,x0) <a href="#_sub3" class="code" title="subfunction x = prox_logistic_multitask(C,Cp,z,gamma,x0,m,n,args,data_weights)">prox_logistic_multitask</a>(C,Cp,x,gamma,x0,m,n,hlp_struct2varargin(lbfgsOptions),data_weights);
0512         lossfunc.eval = @(x,lambda) lambda*<a href="#_sub4" class="code" title="subfunction obj = obj_logistic_multitask(C,x,m,n,data_weights)">obj_logistic_multitask</a>(C,x,m,n,data_weights);
0513     <span class="keyword">case</span> <span class="string">'squared'</span>
0514         <span class="comment">% append a bias to the design matrix</span>
0515         <span class="keyword">if</span> length(A)&gt;1
0516             error(<span class="string">'Squared loss with for multi-task case not yet fully implemented.'</span>); <span class="keyword">end</span>
0517         Ao = cellfun(@(A)[A ones(size(A,1),1)],A,<span class="string">'UniformOutput'</span>,false);
0518         mm = cellfun(<span class="string">'size'</span>,Ao,1);
0519         nn = cellfun(<span class="string">'size'</span>,Ao,2);
0520         <span class="comment">% choose the right prox operator</span>
0521         <span class="keyword">if</span> solverOptions.rho_update
0522             lossfunc.prox = @(x,gamma,x0) <a href="#_sub11" class="code" title="subfunction x = prox_squared_iter(A,b,rho,z,u,n,x0)">prox_squared_iter</a>(Ao{1},y{1},1/gamma,x,zeros(size(x)),mm,nn,x0);
0523         <span class="keyword">else</span>            
0524             Atb = cellfun(@(Ao,y)Ao'*y,Ao,y,<span class="string">'UniformOutput'</span>,false);
0525             [L,U] = deal(cell(1,nTasks));
0526             <span class="keyword">for</span> t=1:nTasks
0527                 [L{t},U{t}] = <a href="#_sub24" class="code" title="subfunction [L,U] = factor(A, rho)">factor</a>(Ao{t},solverOptions.rho/data_weights(t)); <span class="keyword">end</span>
0528             lossfunc.prox = @(x,gamma,x0) <a href="#_sub5" class="code" title="subfunction x = prox_squared_factored_multitask(A,Atb,L,U,rho,z,u,m,n,data_weights)">prox_squared_factored_multitask</a>(Ao,Atb,L,U,solverOptions.rho,x,zeros(size(x)),mm,nn,data_weights);
0529         <span class="keyword">end</span>
0530         lossfunc.eval = @(x,lambda) lambda*<a href="#_sub6" class="code" title="subfunction obj = obj_squared_multitask(A,b,x,m,n,data_weights)">obj_squared_multitask</a>(Ao,y,x,mm,nn,data_weights);
0531     <span class="keyword">case</span> <span class="string">'hyperbolic-secant'</span>
0532         <span class="comment">% lossfunc.prox = @(x,gamma,x0) prox_hs(y,x,gamma,x0,hlp_struct2varargin(lbfgsOptions));</span>
0533         <span class="comment">% lossfunc.eval = @(x,lambda) lambda*obj_hs(x,b);</span>
0534         error(<span class="string">'Hyperbolic-secant loss is not yet implemented.'</span>);
0535     <span class="keyword">otherwise</span>
0536         error(<span class="string">'Unsupported loss function.'</span>);
0537 <span class="keyword">end</span>
0538 lossfunc.y0 = [];
0539 lossfunc = @(lambda)setfield(setfield(lossfunc,<span class="string">'prox'</span>,@(x,gamma,x0)lossfunc.prox(x,gamma*lambda,x0)),<span class="string">'eval'</span>,@(x)lossfunc.eval(x,lambda)); <span class="comment">%#ok&lt;SFLD&gt;</span>
0540 
0541 
0542 <span class="comment">% ensure that regularizers is a cell array of structs</span>
0543 regularizers = {};
0544 <span class="keyword">if</span> isstruct(regularizersArg)
0545     <span class="keyword">for</span> k=1:length(fieldnames(regularizersArg))
0546         <span class="keyword">if</span> isfield(regularizersArg,[<span class="string">'term'</span> num2str(k)])
0547             regularizers{end+1} = regularizersArg.([<span class="string">'term'</span> num2str(k)]); <span class="keyword">end</span> <span class="comment">%#ok&lt;AGROW&gt;</span>
0548     <span class="keyword">end</span>
0549 <span class="keyword">else</span>
0550     regularizers = regularizersArg;
0551 <span class="keyword">end</span>
0552 
0553 
0554 <span class="comment">% set up the regularization functions one by one</span>
0555 regfuncs = {};
0556 <span class="keyword">for</span> t = 1:length(regularizers)
0557     param = regularizers{t};
0558     <span class="keyword">if</span> ~strcmp(param.arg_selection,<span class="string">'none'</span>)
0559         regfunc = struct();
0560         <span class="keyword">if</span> isfield(param,<span class="string">'weights'</span>)
0561             param.weights2 = param.weights; <span class="keyword">end</span>
0562         param.verbose = max(0,solverOptions.verbose-2);
0563         
0564         <span class="comment">% rename &amp; evaluate the linear operator expressions</span>
0565         <span class="keyword">if</span> ischar(param.A)
0566             <span class="keyword">try</span>
0567                 [a,b,c,d,e,f,g,h] = size(reshape(w(1:sum(n)),weightshape)); <span class="comment">%#ok&lt;ASGLU&gt;</span>
0568                 param.A = eval(param.A);
0569             <span class="keyword">catch</span> e
0570                 env_handleerror(e);
0571                 disp([<span class="string">'This param does not evaluate correctly: '</span>  param.A]);
0572             <span class="keyword">end</span>
0573         <span class="keyword">end</span>
0574         
0575         <span class="comment">% if the linear operator happens to accept weights in the shape of the original features</span>
0576         <span class="comment">% (and the numels are matching) then we reshape the weights to that shape before applying</span>
0577         <span class="comment">% the linear operator</span>
0578         <span class="keyword">try</span>
0579             rA = param.A;
0580             rA(reshape(w(1:sum(n)),weightshape));
0581             param.A = @(x)rA(reshape(x(1:sum(n)),weightshape));
0582         <span class="keyword">catch</span>
0583             <span class="keyword">try</span>
0584                 param.A(w(1:sum(n)));
0585                 param.A = @(x)rA(x(1:sum(n)));
0586             <span class="keyword">catch</span> e
0587                 <span class="comment">% sanity check: if this happens either your linear operator is incorrect or</span>
0588                 <span class="comment">% you need to specify NumberOfElements for this term</span>
0589                 error([<span class="string">'The linear operator '</span> char(param.A) <span class="string">' is not applicable to the weights w. Check for syntax errors and sizes.'</span>]);
0590             <span class="keyword">end</span>
0591         <span class="keyword">end</span>
0592         shape_A_out = size(param.A(w));
0593         
0594         <span class="comment">% if shape for the term is unspecified we assume that it is the output shape of the A</span>
0595         <span class="comment">% operator</span>
0596         <span class="keyword">if</span> isfield(param,<span class="string">'shape'</span>) &amp;&amp; isempty(param.shape)
0597             param.shape = shape_A_out; <span class="keyword">end</span>
0598         <span class="keyword">if</span> isfield(param,<span class="string">'shape'</span>) &amp;&amp; ischar(param.shape)
0599             [a,b,c,d,e,f,g,h] = size(reshape(w(1:sum(n)),weightshape)); <span class="comment">%#ok&lt;ASGLU&gt;</span>
0600             param.shape = eval(param.shape);
0601         <span class="keyword">end</span>
0602         
0603         <span class="comment">% set the A matrix for future reference</span>
0604         <span class="keyword">if</span> isfield(param,<span class="string">'A'</span>)
0605             rA = param.A;
0606         <span class="keyword">else</span>
0607             rA = @(x)x(1:sum(n));
0608         <span class="keyword">end</span>
0609         
0610         <span class="comment">% move the A parameter into regfunc.L (handled by ADMM)</span>
0611         <span class="keyword">if</span> isfield(param,<span class="string">'A'</span>) &amp;&amp; ~(strcmp(param.arg_selection,<span class="string">'l2'</span>) &amp;&amp; param.nonorthogonal_transform)
0612             regfunc.L = param.A;
0613             <span class="comment">% remove fields from param</span>
0614             param = rmfield(param,<span class="string">'A'</span>);
0615             <span class="keyword">if</span> isfield(param,<span class="string">'At'</span>)
0616                 param = rmfield(param,<span class="string">'At'</span>); <span class="keyword">end</span>
0617         <span class="keyword">else</span>
0618             regfunc.L = @(x)x(1:sum(n));
0619         <span class="keyword">end</span>
0620         
0621         <span class="comment">% now turn .L into a matrix (since we actually need it in matrix form)</span>
0622         <span class="comment">% the calculation is cached since it's quite slow for large parameter spaces</span>
0623         regfunc.L = operator_to_matrix(regfunc.L,numel(w));
0624         
0625         regfunc.param = param;
0626         
0627         vec = @(x)x(:);
0628         <span class="keyword">if</span> isfield(param,<span class="string">'weights'</span>)
0629             <span class="keyword">if</span> isempty(param.weights)
0630                 param.weights = ones(prod(shape_A_out),1); <span class="keyword">end</span>
0631             shaped_weights = reshape(param.weights,shape_A_out);
0632         <span class="keyword">else</span>
0633             shaped_weights = ones(shape_A_out);
0634         <span class="keyword">end</span>
0635         <span class="keyword">switch</span> param.arg_selection
0636             <span class="keyword">case</span> <span class="string">'l1'</span>
0637                 <span class="keyword">if</span> (isempty(param.weights) || all(param.weights(:)==1)) &amp;&amp; ~isfield(param,<span class="string">'A'</span>)
0638                     regfunc.prox = @(x,gamma,x0) <a href="#_sub13" class="code" title="subfunction x = prox_l1_simple(z, gamma)">prox_l1_simple</a>(x,gamma);
0639                     regfunc.eval = @(x,lambda) lambda*sum(abs(x));
0640                 <span class="keyword">else</span>
0641                     regfunc.prox = @(x,gamma,x0) prox_l1(x,gamma,param);
0642                     regfunc.eval = @(x,lambda) lambda*norm(vec(shaped_weights.*rA(x)),1);
0643                 <span class="keyword">end</span>
0644             <span class="keyword">case</span> <span class="string">'l2'</span>
0645                 <span class="keyword">if</span> isempty(param.y)
0646                     param.y = zeros(prod(shape_A_out),1); <span class="keyword">end</span>
0647                 <span class="keyword">if</span> isfield(param,<span class="string">'A'</span>)
0648                     A = operator_to_matrix(param.A,sum(n));
0649                     regfunc.prox = @(x,gamma,x0) <a href="#_sub11" class="code" title="subfunction x = prox_squared_iter(A,b,rho,z,u,n,x0)">prox_squared_iter</a>(A,param.y,1/gamma,x,zeros(size(x)),sum(n),x0);
0650                     regfunc.eval = @(x,lambda) lambda*<a href="#_sub12" class="code" title="subfunction obj = obj_squared(A,b,x)">obj_squared</a>(A,param.y,x(1:sum(n)));
0651                 <span class="keyword">else</span>                
0652                     regfunc.prox = @(x,gamma,x0) <a href="#_sub15" class="code" title="subfunction x = prox_l2_simple(z, gamma)">prox_l2_simple</a>(x,gamma);
0653                     regfunc.eval = @(x,lambda) lambda*norm(shaped_weights(:).*(vec(rA(x)) - param.y(:)),2).^2;
0654                 <span class="keyword">end</span>
0655             <span class="keyword">case</span> <span class="string">'l1/l2'</span>
0656                 <span class="keyword">if</span> isfield(param,<span class="string">'A'</span>)
0657                     error(<span class="string">'The linear operator for the group sparsity prox operator is not implemented. You can however apply it by using the SDMM algorithm.'</span>); <span class="keyword">end</span>
0658                 <span class="keyword">if</span> isempty(param.g_d) &amp;&amp; isempty(param.g_t) &amp;&amp; (isempty(param.weights2)||all(param.weights2(:)==1)) &amp;&amp; (isempty(param.weights1)||all(param.weights1(:)==1))
0659                     regfunc.prox = @(x,gamma,x0) <a href="#_sub14" class="code" title="subfunction x = prox_l12_simple(z, gamma, shape)">prox_l12_simple</a>(x,gamma,shape_A_out);
0660                     regfunc.eval = @(x,lambda)lambda*<a href="#_sub18" class="code" title="subfunction n = norm_l12_simple(z, shape)">norm_l12_simple</a>(rA(x),shape_A_out);
0661                 <span class="keyword">else</span>                    
0662                     <span class="keyword">if</span> isempty(param.g_d) &amp;&amp; isempty(param.g_t)
0663                         param.g_d = (1:prod(shape_A_out));
0664                         param.g_t = shape_A_out(1)*ones(1,prod(shape_A_out(2:end)));
0665                     <span class="keyword">end</span>
0666                     <span class="keyword">if</span> isempty(param.weights1)
0667                         param.weights1 = ones(numel(param.g_t),1); <span class="keyword">end</span>
0668                     <span class="keyword">if</span> isempty(param.weights2)
0669                         param.weights2 = ones(prod(shape_A_out),1); <span class="keyword">end</span>
0670                     regfunc.prox = @(x,gamma,x0) prox_l12(x,gamma,param);
0671                     regfunc.eval = @(x,lambda) lambda*norm_l12(rA(x),param.g_d,param.g_t,param.weights2,param.weights1);
0672                 <span class="keyword">end</span>
0673             <span class="keyword">case</span> <span class="string">'l1/linf'</span>
0674                 <span class="keyword">if</span> isfield(param,<span class="string">'A'</span>)
0675                     error(<span class="string">'The linear operator for the group sparsity prox operator is not implemented. You can however apply it by using the SDMM algorithm.'</span>); <span class="keyword">end</span>
0676                 <span class="keyword">if</span> isempty(param.g_d) &amp;&amp; isempty(param.g_t) &amp;&amp; (isempty(param.weights2)||all(param.weights2(:)==1)) &amp;&amp; (isempty(param.weights1)||all(param.weights1(:)==1))
0677                     regfunc.prox = @(x,gamma,x0) <a href="#_sub16" class="code" title="subfunction x = prox_l1inf_simple(z, gamma, shape)">prox_l1inf_simple</a>(x,gamma,shape_A_out);
0678                     regfunc.eval = @(x,lambda)lambda*<a href="#_sub19" class="code" title="subfunction n = norm_l1inf_simple(z, shape)">norm_l1inf_simple</a>(rA(x),shape_A_out);
0679                 <span class="keyword">else</span>                    
0680                     <span class="keyword">if</span> isempty(param.g_d) &amp;&amp; isempty(param.g_t)
0681                         param.g_d = (1:prod(shape_A_out));
0682                         param.g_t = shape_A_out(1)*ones(1,prod(shape_A_out(2:end)));
0683                     <span class="keyword">end</span>
0684                     <span class="keyword">if</span> isempty(param.weights1)
0685                         param.weights1 = ones(numel(param.g_t),1); <span class="keyword">end</span>
0686                     <span class="keyword">if</span> isempty(param.weights2)
0687                         param.weights2 = ones(prod(shape_A_out),1); <span class="keyword">end</span>
0688                     regfunc.prox = @(x,gamma,x0) prox_l1inf(x,gamma,param);
0689                     regfunc.eval = @(x,lambda) lambda*norm_l1inf(rA(x),param.g_d,param.g_t,param.weights2,param.weights1);
0690                 <span class="keyword">end</span>
0691             <span class="keyword">case</span> <span class="string">'tv2d'</span>
0692                 <span class="keyword">if</span> isfield(param,<span class="string">'A'</span>)
0693                     error(<span class="string">'The linear operator for the total-variation prox operator is not implemented. You can however apply it by using the SDMM algorithm.'</span>); <span class="keyword">end</span>
0694                 regfunc.prox = @(x,gamma,x0) prox_tv(x,gamma,param);
0695                 regfunc.eval = @(x,lambda) lambda*tv_norm(rA(x),param.shape);
0696             <span class="keyword">case</span> <span class="string">'tv3d'</span>
0697                 <span class="keyword">if</span> isfield(param,<span class="string">'A'</span>)
0698                     error(<span class="string">'The linear operator for the total-variation prox operator is not implemented. You can however apply it by using the SDMM algorithm.'</span>); <span class="keyword">end</span>
0699                 regfunc.prox = @(x,gamma,x0) prox_tv3d(x,gamma,param);
0700                 regfunc.eval = @(x,lambda) lambda*tv_norm3d(rA(x),param.shape);
0701             <span class="keyword">case</span> <span class="string">'trace'</span>
0702                 regfunc.prox = @(x,gamma,x0) <a href="#_sub17" class="code" title="subfunction x = prox_nuclear_simple(z, gamma, shape)">prox_nuclear_simple</a>(x,gamma,shape_A_out);
0703                 regfunc.eval = @(x,lambda) lambda*<a href="#_sub20" class="code" title="subfunction n = norm_nuclear_simple(z, shape)">norm_nuclear_simple</a>(rA(x),shape_A_out);
0704             <span class="keyword">otherwise</span>
0705                 error(<span class="string">'Unrecognized regularization type requested.'</span>);
0706         <span class="keyword">end</span>
0707         regfunc.y0 = [];
0708         regfuncs{end+1} = @(lambda) setfield(setfield(regfunc,<span class="string">'prox'</span>,@(x,gamma,x0)regfunc.prox(x,gamma*lambda,x0)),<span class="string">'eval'</span>,@(x)regfunc.eval(x,lambda)); <span class="comment">%#ok&lt;AGROW,SFLD&gt;</span>
0709     <span class="keyword">end</span>
0710 <span class="keyword">end</span>
0711 
0712 <span class="keyword">if</span> iscell(regweights) &amp;&amp; numel(regweights) == 1
0713     regweights = regweights{1}; <span class="keyword">end</span>
0714 <span class="keyword">if</span> isempty(regweights)
0715     regweights = ones(1,length(regfuncs)); <span class="keyword">end</span>
0716 regweights = regweights/sum(regweights); 
0717 
0718 <span class="comment">% learn the regularization path</span>
0719 <span class="keyword">if</span> verbosity
0720     disp(<span class="string">'solving regularization path...'</span>); <span class="keyword">end</span>
0721 y0 = {};
0722 nLambdas = length(lambdas);
0723 regpath = cell(nLambdas,1);
0724 hist = cell(1,nLambdas);
0725 <span class="keyword">for</span> k =1:nLambdas
0726     
0727     <span class="comment">% set up parameters</span>
0728     termweights = [1,lambdas(k)*regweights];
0729     lossfunc = lossfunc(1);
0730     lossfunc.L = [];
0731     <span class="keyword">if</span> ~isempty(y0)
0732         lossfunc.y0 = y0{1}; <span class="keyword">end</span>
0733     lossfunc.param = struct();
0734     <span class="keyword">for</span> r = 1:length(regfuncs)
0735         tmpregfuncs(r) = regfuncs{r}(termweights(1+r)); <span class="comment">%#ok&lt;AGROW&gt;</span>
0736         <span class="keyword">if</span> ~isempty(y0)
0737             tmpregfuncs(r).y0 = y0{1+r}; <span class="keyword">end</span> <span class="comment">%#ok&lt;AGROW&gt;</span>
0738     <span class="keyword">end</span>
0739     
0740     <span class="comment">% we stash the termweights in the solverOptions because the hlp_diskcache below will by default</span>
0741     <span class="comment">% not parse the tmpregfuncs anonymous function deep enough to discover the termweights, and thus</span>
0742     <span class="comment">% cause cache collisions (this can be resolved by setting the serialize_anonymous_fully option</span>
0743     <span class="comment">% to true, but since that is a global option it could cause unexpected behavior in the rest of</span>
0744     <span class="comment">% BCILAB)</span>
0745     solverOptions.termweights = termweights;
0746     
0747     <span class="comment">% solve</span>
0748     t0 = tic;
0749     <span class="keyword">if</span> verbosity
0750         fprintf(<span class="string">'  scanning lambda = %f (%i/%i)...'</span>,lambdas(k),k,nLambdas); <span class="keyword">end</span>
0751     <span class="keyword">if</span> solverOptions.warmstart
0752         [w,y0,rho,hist{k}] = hlp_diskcache(<span class="string">'intermediate'</span>,@consensus_admm,w,[lossfunc tmpregfuncs],solverOptions); <span class="comment">%#ok&lt;ASGLU&gt;</span>
0753     <span class="keyword">else</span>
0754         [w,y0dummy,rho,hist{k}] = hlp_diskcache(<span class="string">'intermediate'</span>,@consensus_admm,zeros(size(w)),[lossfunc tmpregfuncs],solverOptions); <span class="comment">%#ok&lt;ASGLU&gt;</span>
0755     <span class="keyword">end</span>
0756     <span class="keyword">if</span> verbosity
0757         fprintf(<span class="string">' %i iters; t = %.1fs\n'</span>,length(hist{k}.objval),toc(t0)); <span class="keyword">end</span>
0758     
0759     <span class="comment">% assemble output weights for each task</span>
0760     <span class="keyword">if</span> includebias &amp;&amp; nTasks &gt; 1
0761         offsets = cumsum([1 n(1:end-1)]);
0762         <span class="keyword">for</span> t=1:nTasks
0763             regpath{k}{t} = w([offsets(t)+(0:n(t)-1) end-length(m)+t]); <span class="keyword">end</span>
0764     <span class="keyword">else</span>
0765         regpath{k}{1} = w;
0766     <span class="keyword">end</span>
0767 <span class="keyword">end</span>
0768 
0769 [res.regpath,res.hist] = deal(regpath,hist);
0770 
0771 
0772 <span class="comment">% --- multi-task logistic loss code ---</span>
0773 
0774 <a name="_sub2" href="#_subfunctions" class="code">function [val,grad] = obj_proxlogistic_multitask(x,C,Cp,z,gamma,m,n,data_weights)</a>
0775 <span class="comment">% objective function for the multi-task logistic loss proximity operator (effectively l2-regularized logreg)</span>
0776 offsets = cumsum([1 n(1:end-1)]);
0777 <span class="keyword">for</span> t=length(m):-1:1
0778     <span class="comment">% move bias from end to inline</span>
0779     xt = x([offsets(t)+(0:n(t)-1) end-length(m)+t]);
0780     zt = z([offsets(t)+(0:n(t)-1) end-length(m)+t]);
0781     ecx = exp(C{t}*xt);
0782     scaling = (gamma*data_weights(t)/m(t));
0783     val{t} = (1/2)*sum((xt-zt).^2) + scaling*gather(sum(log1p(ecx)));
0784     <span class="keyword">if</span> ~isfinite(val{t})
0785         ecx(~isfinite(ecx(:))) = 2.^50;
0786         val{t} = (1/2)*sum((xt-zt).^2) + scaling*gather(sum(log1p(ecx)));
0787     <span class="keyword">end</span>
0788     grad{t} = (xt - zt) + scaling*gather(Cp{t}*(ecx./(1+ecx)));    
0789 <span class="keyword">end</span>
0790 val = sum([val{:}]);
0791 grad = vertcat(grad{:});
0792 <span class="comment">% move biases back to end</span>
0793 grad = [grad;grad(cumsum(n+1))]; grad(cumsum(n+1)) = [];
0794 
0795 <a name="_sub3" href="#_subfunctions" class="code">function x = prox_logistic_multitask(C,Cp,z,gamma,x0,m,n,args,data_weights)</a>
0796 x = liblbfgs(@(w)<a href="#_sub2" class="code" title="subfunction [val,grad] = obj_proxlogistic_multitask(x,C,Cp,z,gamma,m,n,data_weights)">obj_proxlogistic_multitask</a>(w,C,Cp,z,gamma,m,n,data_weights),x0,args{:});
0797 
0798 <a name="_sub4" href="#_subfunctions" class="code">function obj = obj_logistic_multitask(C,x,m,n,data_weights)</a>
0799 obj = 0;
0800 offsets = cumsum([1 n(1:end-1)]);
0801 <span class="keyword">for</span> t=1:length(m)
0802     xt = x([offsets(t)+(0:n(t)-1) end-length(m)+t]);
0803     obj = obj + gather(sum(log1p(exp(C{t}*xt))))*(data_weights(t)/m(t)); 
0804 <span class="keyword">end</span>
0805 
0806 
0807 <span class="comment">% --- multi-task square loss code  ---</span>
0808 
0809 <a name="_sub5" href="#_subfunctions" class="code">function x = prox_squared_factored_multitask(A,Atb,L,U,rho,z,u,m,n,data_weights)</a>
0810 <span class="comment">% this version can only be used if rho stays constant (TODO: confirm the use of data_weights as correct)</span>
0811 scaling = rho/data_weights;
0812 offsets = cumsum([1 n(1:end-1)]);
0813 <span class="keyword">for</span> t=length(m):-1:1
0814     q = Atb{t} + scaling(t)*(z([offsets(t)+(0:n(t)-1) end-length(m)+t]) - u([offsets(t)+(0:n(t)-1) end-length(m)+t]));
0815     <span class="keyword">if</span>(m(t) &gt;= n(t))
0816         x{t} = U{t}\(L{t}\q);
0817     <span class="keyword">else</span>
0818         x{t} = q/scaling(t) - (A{t}'*(U{t}\(L{t}\(A{t}*q))))/scaling(t)^2;
0819     <span class="keyword">end</span>
0820 <span class="keyword">end</span>
0821 x = vertcat(x{:});
0822 x = [x;x(cumsum(n+1))]; x(cumsum(n+1)) = [];
0823     
0824 <a name="_sub6" href="#_subfunctions" class="code">function obj = obj_squared_multitask(A,b,x,m,n,data_weights)</a>
0825 obj = 0;
0826 <span class="keyword">for</span> t=length(m):-1:1
0827     xt = x([offsets(t)+(0:n(t)-1) end-length(m)+t]); 
0828     obj = obj + data_weights(t)*0.5*norm(A*xt - b,2).^2;
0829 <span class="keyword">end</span>
0830 
0831 
0832 <span class="comment">% --- logistic loss code ---</span>
0833 
0834 <a name="_sub7" href="#_subfunctions" class="code">function [val,grad] = obj_proxlogistic(x,C,Cp,z,gamma,m)</a>
0835 <span class="comment">% objective function for the logistic loss proximity operator (effectively l2-regularized logreg)</span>
0836 ecx = exp(C*x);
0837 scaling = (gamma/m);
0838 val = (1/2)*sum((x-z).^2) + scaling*gather(sum(log1p(ecx)));
0839 <span class="keyword">if</span> ~isfinite(val)
0840     ecx(~isfinite(ecx(:))) = 2.^50;
0841     val = (1/2)*sum((x-z).^2) + scaling*gather(sum(log1p(ecx)));
0842 <span class="keyword">end</span>
0843 grad = (x - z) + scaling*gather(Cp*(ecx./(1+ecx)));
0844 
0845 <a name="_sub8" href="#_subfunctions" class="code">function x = prox_logistic(C,Cp,z,gamma,x0,m,args)</a>
0846 x = liblbfgs(@(w)<a href="#_sub7" class="code" title="subfunction [val,grad] = obj_proxlogistic(x,C,Cp,z,gamma,m)">obj_proxlogistic</a>(w,C,Cp,z,gamma,m),x0,args{:});
0847 
0848 <a name="_sub9" href="#_subfunctions" class="code">function obj = obj_logistic(C,x,m)</a>
0849 obj = gather(sum(log1p(exp(C*x))))/m;
0850 
0851            
0852 <span class="comment">% --- square loss code  ---</span>
0853 
0854 <a name="_sub10" href="#_subfunctions" class="code">function x = prox_squared_factored(A,Atb,L,U,rho,z,u,m,n)</a>
0855 <span class="comment">% this version can only be used if rho stays constant</span>
0856 q = Atb + rho*(z - u);
0857 <span class="keyword">if</span>(m &gt;= n)
0858     x = U\(L\q);
0859 <span class="keyword">else</span>
0860     x = q/rho - (A'*(U\(L\(A*q))))/rho^2;
0861 <span class="keyword">end</span>
0862 
0863 <a name="_sub11" href="#_subfunctions" class="code">function x = prox_squared_iter(A,b,rho,z,u,n,x0)</a>
0864 [x, flag, relres, iters] = lsqr([A; sqrt(rho)*speye(n)], [b; sqrt(rho)*(z-u)], [], [], [], [], x0); <span class="comment">%#ok&lt;NASGU,ASGLU&gt;</span>
0865 
0866 <a name="_sub12" href="#_subfunctions" class="code">function obj = obj_squared(A,b,x)</a>
0867 obj = 0.5*norm(A*x - b,2).^2;
0868 
0869 
0870 <span class="comment">% --- some useful prox operators &amp; norms ---</span>
0871 
0872 <a name="_sub13" href="#_subfunctions" class="code">function x = prox_l1_simple(z, gamma)</a>
0873 <span class="comment">% for the l1 norm</span>
0874 x = max(0,z-gamma) - max(0,-z-gamma);
0875 
0876 <a name="_sub14" href="#_subfunctions" class="code">function x = prox_l12_simple(z, gamma, shape)</a>
0877 <span class="comment">% for the columnwise group l1/l2 norm</span>
0878 z = reshape(z,shape);
0879 x = bsxfun(@times,max(0,1-gamma./sqrt(sum(z.^2))),z);
0880 x = x(:);
0881 
0882 <a name="_sub15" href="#_subfunctions" class="code">function x = prox_l2_simple(z, gamma)</a>
0883 <span class="comment">% for the l2 norm</span>
0884 x = bsxfun(@times,max(0,1-gamma./sqrt(sum(z.^2))),z);
0885 
0886 <a name="_sub16" href="#_subfunctions" class="code">function x = prox_l1inf_simple(z, gamma, shape)</a>
0887 <span class="comment">% for the columnwise group l1/linf norm</span>
0888 z = reshape(z,shape);
0889 x = bsxfun(@times,max(0,1-gamma/max(abs(z))),z);
0890 x = x(:);
0891 
0892 <a name="_sub17" href="#_subfunctions" class="code">function x = prox_nuclear_simple(z, gamma, shape)</a>
0893 <span class="comment">% for the trace norm on first 2 dimensions</span>
0894 z = reshape(z,shape);
0895 <span class="keyword">if</span> ndims(z)&gt;2
0896     siz = size(z);
0897     z = reshape(z,siz(1),siz(2),[]);
0898     <span class="keyword">for</span> k=1:size(z,3)
0899         [U,S,V] = svd(z(:,:,k),<span class="string">'econ'</span>);
0900         S = diag(max(0,diag(S)-gamma));
0901         z(:,:,k) = U*S*V.';
0902     <span class="keyword">end</span>
0903     x = reshape(z,siz);
0904 <span class="keyword">else</span>
0905     [U,S,V] = svd(z,<span class="string">'econ'</span>);
0906     S = diag(max(0,diag(S)-gamma));
0907     x = U*S*V.';
0908 <span class="keyword">end</span>
0909 x = x(:);
0910 
0911 <a name="_sub18" href="#_subfunctions" class="code">function n = norm_l12_simple(z, shape)</a>
0912 <span class="comment">% for the columnwise group l1/l2 norm</span>
0913 n = sqrt(sum(reshape(z.^2,shape)));
0914 n = sum(n(:));
0915 
0916 <a name="_sub19" href="#_subfunctions" class="code">function n = norm_l1inf_simple(z, shape)</a>
0917 <span class="comment">% for the columnwise group l1/linf norm</span>
0918 n = max(reshape(abs(z),shape));
0919 n = sum(n(:));
0920 
0921 <a name="_sub20" href="#_subfunctions" class="code">function n = norm_nuclear_simple(z, shape)</a>
0922 <span class="comment">% for the trace norm on first 2 dimensions</span>
0923 z = reshape(z,shape);
0924 <span class="keyword">if</span> ndims(z)&gt;2
0925     siz = size(z);
0926     z = reshape(z,siz(1),siz(2),[]);
0927     n = 0;
0928     <span class="keyword">for</span> k=1:size(z,3)
0929         n = n+sum(svd(z(:,:,k))); <span class="keyword">end</span>
0930 <span class="keyword">else</span>
0931     n = sum(svd(z));
0932 <span class="keyword">end</span>
0933 
0934 
0935 <span class="comment">% -- hyperbolic secant distribution loss code ---</span>
0936 
0937 <a name="_sub21" href="#_subfunctions" class="code">function [val,grad] = obj_proxhs(x,b,z,gamma)</a>
0938 <span class="comment">% objective function for the hyperbolic-secant loss proximity operator</span>
0939 zz = x-b;
0940 mz = abs(zz);
0941 ezzmz = exp(zz-mz);
0942 enzzmz = exp(-zz-mz);
0943 val = (1/2)*norm(x - z).^2 + gamma*sum(mz + log(ezzmz+enzzmz)-log(2/pi));
0944 grad = (x - z) + gamma*(ezzmz-enzzmz)./(ezzmz+enzzmz);
0945 
0946 <a name="_sub22" href="#_subfunctions" class="code">function x = prox_hs(b,z,gamma,x0,args)</a>
0947 x = liblbfgs(@(w)<a href="#_sub21" class="code" title="subfunction [val,grad] = obj_proxhs(x,b,z,gamma)">obj_proxhs</a>(w,b,z,gamma),x0,args{:});
0948 
0949 <a name="_sub23" href="#_subfunctions" class="code">function obj = obj_hs(x,b)</a>
0950 x = x-b;
0951 ax = abs(x);
0952 obj = sum(mz + log(exp(x-ax)+exp(-x-ax))-log(2/pi));
0953 
0954 
0955 <span class="comment">% --- helper functions ---</span>
0956 
0957 <a name="_sub24" href="#_subfunctions" class="code">function [L,U] = factor(A, rho)</a>
0958 <span class="comment">% note: rho is 1/gamma</span>
0959 [m,n] = size(A);
0960 <span class="keyword">if</span> (m &gt;= n)
0961     L = chol(A'*A + rho*speye(n),<span class="string">'lower'</span>);
0962 <span class="keyword">else</span>
0963     L = chol(speye(m) + 1/rho*(A*A'),<span class="string">'lower'</span>);
0964 <span class="keyword">end</span>
0965 <span class="comment">% force matlab to recognize the upper / lower triangular structure</span>
0966 L = sparse(L);
0967 U = sparse(L');
0968 
0969</pre></div>

<hr><address>Generated on Wed 19-Aug-2015 18:06:23 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>